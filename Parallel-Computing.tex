\documentclass[10pt]{article}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{setspace}
\usepackage{ragged2e}
\usepackage{color}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{array}
\usepackage{makecell}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xfrac}
\usepackage{etoolbox}
\usepackage{cite}
\usepackage{url}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{url}
\usepackage{esvect}
\usepackage{commath}
\usepackage{verbatim} % for block comments
\usepackage{enumitem}
\usepackage{hyperref} % for clickable table of contents
\usepackage{braket}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{gensymb}
\usepackage{longtable}
\usepackage{soul} % for striking out text
\usepackage{tcolorbox} % for colored boxes
\tcbuselibrary{breakable} % to allow colored boxed to extend over multiple pages
\usepackage[makeroom]{cancel}	% to cancel out text
\usepackage{breqn}
\usepackage[mathscr]{euscript}
\usepackage{listings}

% for circled numbers
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}


\titleclass{\subsubsubsection}{straight}[\subsection]

% define new command for triple sub sections
\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\newcommand{\volume}{\mathop{\ooalign{\hfil$V$\hfil\cr\kern0.08em--\hfil\cr}}\nolimits}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

% Generate the glossary of acronyms
\usepackage[acronym]{glossaries}
\makeglossaries

\newacronym{ai}{AI}{Arithmetic Intensity}
\newacronym{avx}{AVX}{Advanced Vector Extensions}
\newacronym{blas}{BLAS}{Basic Linear Algebra Subroutines}
\newacronym{clump}{CLUMP}{Cluster of Symmetric Multiprocessor}
\newacronym{csr}{CSR}{Compressed Sparse Row}
\newacronym{dram}{DRAM}{Dynamic Random Access Memory}
\newacronym{fma}{FMA}{Fused Multiply-Add}
\newacronym{ge]{GE}{Gaussian Elimination}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{ilp}{ILP}{Instruction Level Programming}
\newacronym{mpi}{MPI}{Message Passing Interface}
\newacronym{ni}{NI}{Network Interface}
\newacronym{simd}{SIMD}{Single Instruction, Multiple Data}
\newacronym{smp}{SMP}{Symmetric Multiprocessor}
\newacronym{spmv}{SpMV}{Sparse Matrix-Vector Multiply}
\newacronym{sram}{SRAM}{Second Random Access Memory}
\newacronym{sse}{SSE}{Streaming SIMD Extensions}
\newacronym{tlb}{TLB}{Translation Look-Aside Buffer}
\newacronym{uma}{UMA}{Uniform Memory Access}
\newacronym{upc}{UPC}{Unified Parallel C}

\begin{document}

\begin{centering}
\Large CS 267: Applications of Parallel Computing\\
\end{centering}

\tableofcontents
\clearpage

\section{Introduction}
\begin{flushleft}\justify

This document contains my course notes for CS 267, as well as my notes on general topics such as computer architecture.

\section{Common Terminology and Tools}

A flop is a floating point operation, which is usually in double precision unless otherwise noted. While the fastest computer has a clock rate of 55 Pflop/s, most laptops are in the Gflop/s range. 

A node is a multicore processor. While there are a certain number of physical cores per CPU, \textit{virtual cores} refer to the multiplication of the number of physical cores with the number of CPUs on a node? Many cores per CPU is not necessarily advantageous, since it requires the programmer to write algorithms with a very high degree of parallelism.

Leadership machines are computers that are designed to get the best performance possible, though they might not be as user-friendly.

Threading occurs when you have more than one \gls{mpi} task per core. 

task? 
nodes?
Why do we have to specify the CPU frequency?

A stiff matrix has large differences between the largest and smallest entries, which means that a vector that is acted upon by \(A\) will change rapidly, which makes explicit methods less accurate (cannot capture sharp gradients as well). 

\subsection{VTune}

This software helps you profile your code - this is better than running the profilers on debug-mode code, since the optimization differs between debug code and fully-optimized code. Do profiling on the final executable.

\subsection{Benchmarks}

All of the fastest computers in the world are evaluated based on their performance with regard to several benchmarks, most famous of which is the Linpack benchmark. This benchmark asks the computers to solve a dense \(Ax=b\) multiply, and report peak speeds. While this benchmark is fairly representative of overall performance, it should be noted that a computer's peak speed is faster than the speed at which the Linpack benchmark is completed, and is sometimes two times as fast. The Gordon Bell prize is given to the faster Pflop/s speed of a machine on any task.

The peak speed is currently around 100 Pflop/s, and the total computing speed of the top 500 list is approaching an Exaflop/s speed. The faster computer accounts for roughly 10\% of the sum of the top 500. The average system age on the top 500 list has been increasing in the past several years, which shows that the performance is starting to slow in the industry.

\section{A Motivation for Parallel Computing}

Up until around 2005, many did not believe that parallel computing would succeed, and many of the early startups attempting parallel computing failed due to Moore's law. Moore's law, hypothesized by one of the cofounders of Intel, predicted that the transistor density on a computer chip would double every 1.5 years as the transistor size shrinks. Shrinking a transistor size by a factor of \(x\) results in a factor of \(x\) increase in the clock speed since the wires connecting the transistors are shorter. In addition, due to manufacturing improvements, the die size, or the size of silicon that is allocated to each computer chip, has also increased by a factor of \(x\). Decreasing the size of the transistor then also leads to an increase in the number of transistors per unit area by a factor of \(x^2\). Overall, this leads to an \(x^4\) improvement in computational speed every 1.5 years without any effort required for modification of a serial program. The actual performance observed improved by a factor of \(x^3\), however. This improvement was realized by 1) improving the on-chip parallelism with \gls{ilp}, where parallelism is implemented on each chip itself, and 2) by increasing locality by building caches, which reduces data movement. More transistors on a chip, however, does not necessarily lead to faster processors. We wanted to take advantage of having more transistors on a chip, so we added speculation into the processor logic - the program would guess where the execution would go next and would then go preemptively execute a header somewhere. In some cases, this wastes power.

However, this improvement in computational speed of serial programs could not continue forever due to several important limitations.

\begin{itemize}
\item Manufacturing limits - only with extremely high probability would all 8 processors on an 8-core chip function correctly - some chips were sold with 7/8 processors guaranteed to function
\item Cost to build a chip increases exponentially
\item Power density continues to increase, which makes the chips difficult to cool. Power scales as \(V^2fC\), where \(V\) is the voltage, \(f\) the frequency, and \(C\) the number of processors. Power increases as \(x^4\) with Moore's law, and hence the power density had to be reduced by lowering the clock speed.
\end{itemize}

Experts predict that Moore's law will continue through the early 2020s, but will then level out. Improvements in the clock speed have already begun to level out. So, the accommodate demands for improved computing, the number of cores on a computer chip has begun to increase. So, instead of getting faster processors in terms of frequency, we're getting more cores, which requires programming to be done in a parallel setting. Today, all major computer vendors produce multicore chips, and it's really not even possible to buy a sequential computer anymore.

\subsection{Challenges to Parallel Computing}

There are several key challenges that face the ability of parallel computing to speed up our computations. First, there must be enough parallelism in the task at hand. Some tasks in a program will be inherently serial, and those may turn out to be severe bottlenecks. Amdahl's law predicts the speedup, or reduction in runtime when using \(P\) processors instead of 1 processor, for \(s\) the fraction of the program that must be run in serial.

\begin{equation}
\textrm{Speedup}=\frac{\textrm{Time(1)}}{\textrm{Time(P)}}\leq\frac{1}{s/1+(1-s)/P}\leq\frac{1}{s}
\end{equation}

Second, the degree of parallelism must be sufficiently granular to amortize the cost of parallel programming. Moving data costs significantly more than arithmetic, so the chunks of parallel actions must be sufficiently large. However, the parallel chunks should not be too large that there is not enough parallel work. Parallel programming incurs overhead due to:

\begin{itemize}
\item Cost of starting a thread or process
\item 
\end{itemize}

\section{Single Processor Machines}
 
Even if you're running on a single processor, chances are you're not going to be operating at the peak performance speed, or the guaranteed-to-not-exceed speed advertised by the manufacturer and computed as the multiplication of the flops/cycle with the clock speed and number of nodes. Most applications run at around 10\% of the peak performance (flops/cycle * clock rate * processors). So, before parallelizing code, you should first try to run as fast as possible on one processor before splitting up operations among multiple processors. Most of this lost performance (both time and energy) is due to interacting with the memory system, moving data between different locations. 

The compiler will try to generate optimal code based on your hardware, but in practice the compiler does not know what is the best way to optimize. 

\subsection{Costs in Modern Processors}

Processors name bytes and words in its address space. These represents integers, floats, arrays, etc. Operations include read/write on this information, arithmetic, and logical operations. Each of the arithmetic and logical operations has \textit{roughly} the same cost, but the read and write operations are much more expensive. The compiler translates the human-typed code into assembly language that is understood by the computer (the code that is run on the processor), which adds in the read/write commands that are not explicitly written in human-typed code. For instance, to use a variable in a program, I simply type the name of that variable - I don't write the instructions to go find it in memory and then read it. Likewise, I use equal signs to write values. You could write assembly language to influence the registers, but this is typically done by the compiler. The compiler performs register allocation, which is the process by which information read from memory is written to a physical data structure called a register. All information to be used by the processor must first be loaded into a register. Registers are physical quantities that have very small, very fast memory to hold variables that are actually being used by the processor; they are located adjacent to the processor, on the computer chip. Then, arithmetic and logical operations are performed on the registers, where output results are written to registers (?) before being moved to other locations in memory if needed. By tailoring your program, you can encourage the compiler to act in a certain way with the memory system.

Processors also have caches, which are small amounts of fast memory that are physically located close to the processor on the computer chip, or off the chip. The caches are controlled by hardware, not by software (?). The hardware moves information into the caches, which can dramatically impact read operations. The cache is used to store values of recently used or nearby data that will not fit into the registers. When you actually need data, you move it from memory to cache, and then from cache to register. 

\subsection{Memory Hierarchies}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{figures/memory-heirarchy.pdf}
\caption{Memory hierarchy in a single processor.}
\end{figure}

\begin{itemize}
\item The \textbf{processor} is the part of the computer that performs actions. 
	\begin{itemize}
	\item Directly on the processor are \textbf{registers}, which are used when presently performing operations on variables. These registers are typically 16 bytes. The cost of register access is essentially zero.
	\item The processor typically also has an on-chip cache, referred to as the L1-cache (level-1 cache), or simply the cache. The size of the L1-cache is on the order of 16 kB, and is limited in size because it has to fit on the chip. The cost of L1-cache access takes about 1-2 cycles.
	\end{itemize}
\item Off the chip is the \gls{sram}, or other caches that are typically referred to as the L2-cache (\(\sim\)2 MB), L3-cache, etc., though some of these caches could also be on the chip. A large cache will always have delays, since hardware has to check longer addresses and due to associativity rules. More generally, the further from the chip, the greater the time delays due to the transit times through longer wires. The Cray T3E eliminated one cache to speed up misses of the cache. When a cache miss occurs, you waste time moving through the memory hierarchy to get to main memory. If you have a program that is well-optimized to the caches closer to the processor, then you don't need some of the further-away caches. IBM uses a ``victim cache,'' so that data in the cache that is about to be thrown away due to new incoming data that is to be stored in the same cache location can be stored in the victim cache. This is overall cheaper than sending back to \gls{dram} again.
\item \gls{dram}
\item Disk. Supercomputers don't have a disk attached, so they do not have any virtual memory (?). Clouds, on the other hand, do have virtual memory.
\item Supercomputers have tertiary storage, where robots move \textbf{disks/tape} into computers for reading.
\end{itemize}

Historically, while the CPU has been improving according to Moore's law, the DRAM latency (time for data to move off-chip and onto DRAM) has been improving at a much slower rate, so these memory hierarchies are getting deeper. We want to develop algorithms that touch memory as infrequently as possible. 

Cache is fast (expensive) memory that keeps a \textit{copy} of data, and the information about where it came from, in main memory; it is hidden from software. For data at memory address xxxxx1101, it will be stored at cache location 1101. So, every address location that ends with 1101 will be mapped to this same cache location (for a direct-mapped cache). When data is read in from memory, it is moved into the cache so that the next time you access that value it is faster. A cache hit occurs when, upon requesting a value for a register, that value is already stored in the cache. Alternatively, a cache miss occurs when that data is not already in the cache, and you have to go out further in memory to obtain it. Then, the hardware will store the requested data in cache and eliminate whatever was previously in cache to avoid such a miss in the future. With parallel programming, taking advantage of the fact that data is moved into the cache may be more difficult if the data is needed on every processor, since then the data movement which would only have to be performed one time has to be performed once for each processor. But, sometimes it is still worth it for processors to perform duplicate actions so that they don't need to communicate information.

The cache line length is the number of bytes loaded together in one entry - caches always load more than one value at a time, even though you may only request one line; this takes advantage of spatial locality. Loading in a single value will load in its \(n\) neighbors for a cache line length of \(n\). A direct-mapped cache can store only one address in a given range of cache (all xxxxx1101 values would be stored in cache location 1101). Because this can lead to bad behavior, associative caches, or \(n\)-way caches, which can store \(n\) of the xxxxx1101 locations at cache location 1101. Up to 16 xxxxx1101 entries can be stored in an associative cache. Associative caches reduce collision issues.

Also in the memory hierarchy are several other levels, such as virtual memory. Even if your program will not fit in main memory, some of the program will be put out on disk by providing extra address on the disk, where the hardware system maps those address. The \gls{tlb} determines whether information is held on disk or in main memory. A page is the amount of memory that can be read from disk to main memory in a group, and is 8 kB, and could be determined from a memory benchmark by looking for the plateau on an array size that can be read in to the L2-cache in entirety.

Latency is the time to transfer a single piece of data, and is typically denoted as \(\alpha\). The bandwidth is the speed of data transfer (bytes/second), and because the bandwidth is improving faster than the latency (23\% vs. 7\% per year), we want to use spatial locality and temporal locality to avoid unnecessary data transfer. The inverse bandwidth is typically denoted as \(\beta\), and is the more commonly-used bandwidth metric because it has the same units as latency. Both latency and bandwidth are hardware parameters, and we prefer to be limited by bandwidth instead of latency. We can handle these two aspects of slow memory operations, through the following concepts.

\begin{itemize}
\item Spatial locality is the desire to reduce the need to transfer data by reading in a chunk of data and using it all at the same time so that multiple read/use statements are not required.
\item Temporal locality attempts to reuse an item that was previously accessed so that you don't have to repeatedly access the same value and waste read time. Each time you read a value from memory, it is saved in registers or cache, eliminating another slow memory operation the next time you need it provided it has not been overwritten on the cache - this is known as bandwidth filtering.
\item We can take advantage of the fact that bandwidth is better than latency by allowing the processor to issue multiple reads/writes with a single instruction through vector operations.
\item We can take advantage of better bandwidth by using prefetching, which is the process by which the hardware guesses what you are going to require next. Then, the hardware goes and fetches that information ahead of time, whether or not you actually want to use it next.
\item We can issue writes, and then buffer them, so that they are sent to slow memory at a later stage. Both prefetching and write buffering require that nothing dependent is happening.
\item Because memory is run by separate hardware, you can run a bunch of memory instructions that can be run in parallel while the processor is doing arithmetic. So, the runtime is then the maximum of the arithmetic time and the memory retrieval time.
\end{itemize}

Supercomputers tend to have quieter environments (less background activity) than a laptop, so benchmarks will give much steadier runtime results. 

\subsubsection{Memory Benchmarks}

Data is read in from memory with varying strides and for varying array sizes, the smaller the stride the better, since cache lines larger than or equal to the stride will take advantage of data already read in from the previous stride. If the entire length of the data array fits in the L1-cache, then you will never experience the increase in cost of the further-away caches, and the stride has no impact on the retrieval cost, because when you load the data in once it stays in cache and you don't need to read it in any more times, and the total cost becomes equivalent to the cache hit time (time to access the L1-cache).

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{figures/membench.pdf}
\caption{Memory benchmark results, for a computer with only one level of cache.}
\end{figure}

If the data array does not fit in the L1-cache, then a plateau is reached that signifies the memory cost that is incurred every time there is a cache miss. The cost increases with stride initially because you will be able to reuse some data due to the cache line length, but for large enough strides, you don't get to reuse any of this information. The left corner on the plateau occurs at the cache line length. The right corner on the plateau occurs because the data may again be able to fit in the L1-cache (?).

A second memory benchmark involves the Stanza Triad, which attempts to evaluate the efficiency of prefetching. Read a bunch of consecutive memory locations, then skip a bunch, and then read consecutive again, and so on. The highest bandwidth would occur if we read in consecutive memory locations. The larger the stanza length, the closer we get to this optimal performance. Prefetching gets the next cache line (pipelining) while using the current one. This does not kick in immediately, but instead the performance depends on the length of data being read in. Prefetching will improve your performance if you're using the information just past the cache line you just loaded. Prefetching essentially increases the cache line size, if used successfully.

\subsection{The Roofline Model}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/roofline.pdf}
\caption{The roofline model.}
\end{figure}

The \gls{ai} is the total number of flops/total \gls{dram} bytes moved. Because memory operations are slow, the \gls{ai} should be large for high performance - because this is an algorithmic characteristic, we can tailor this. The goal is to attain as high as possible Gflop/s. \gls{ilp} \gls{simd}, and \gls{fma}, for instance, will raise the highest attainable Gflop/s (up to the peak performance). 

The diagonal line of the plot is determined by the peak \gls{dram} bandwidth. Lack of proper prefetching, for instance, will lower the attainable bandwidth. You want both a high bandwidth and a high \gls{ai}. There is no point trying to improve the arithmetic performance if you are bandwidth-limited.

\subsection{Parallelism within Single Processors}

The parallelism within a single processor is hidden from software. The hardware will automatically try to take advantage of parallelism within a single processor using pipelining and \gls{simd}. Depending on how you write your code, the compiler may or may not optimize correctly using these single-processor parallelism techniques. Compilers have optimization flags that tell the compiler to optimize in certain ways. 

Autotuning is the process by which you simply test every code configuration to figure out which one works best for your application - this was first used in CS 267. ATLAS, a matrix-matrix multiply tuner, is used in Matlab. For some algorithms, autotuning can be performed off-line, instead of at runtime. For example, for matrix multiplication, all that matters is the size of the matrix - the actual entries of the matrix don't have an impact, since a multiplication is always the same cost regardless of the numeric values themselves. Autotuning performs about as well as hand-optimized code.

\subsubsection{Pipelining}

If you have enough stages in an operation, then you can do many things at once. For example, to do several loads of laundry, its best to run the dryer while running the washer for the next load. Pipelining helps bandwidth, but not latency (the time to do a single load of laundry still takes the same length of time). Bandwidth is limited by the slowest part of the pipeline, and the potential speedup is the number of pipe stages. Pipelining is also used within arithmetic units. 

\subsubsection{Single Instruction, Multiple Data}

\gls{simd} is the process by which a single instruction is used to perform an operation on multiple registers at the same time. Using bigger registers, you can load multiple values into these registers and perform a single instruction to act on the multiple values in the register. The compiler will try to find \gls{simd} instructions and use them as much as possible. Because a register contains 16 bytes, \gls{simd} behaves differently if you're using different data types - because doubles are 8 bytes, while floats are 4 bytes, you can only achieve 2x parallelism with doubles, but 4x parallelism with floats. The challenge with \gls{simd} is that the data must be in contiguous memory locations when loaded - this requires a good degree of spatial locality. The instructions also have to move data around from one part of the register to another. With \gls{gpu}s, you can be performing single instructions with thousands of data pieces at the same time, a principle that is similar to \gls{simd} instructions. 

\subsubsection{Fused Multiply-Add}

Single processors also typically have a special operation called a \gls{fma} to perform the multiplication of two numbers and add it to a third in a single instruction. This can be done at the same bandwidth as an add or multiply alone. This is very commonly-used in linear algebra.

\subsection{Matrix Multiplication}

Matrix multiplication is probably the most well-studied algorithm, as it appears very frequently in scientific computing. In addition, matrix multiplication benefits greatly from optimization. A matrix is a 2-D array of elements, but it is stored in memory 1-D. The default in C is to store by rows, and in Fortran is to store by columns. Running down a column of a row-major matrix is very expensive, and hence the programming language used has an important impact on matrix multiplication algorithms. If the matrix size is not a multiple of the cache line size, you can get complicated behavior.

Assuming that there are only two levels of memory hierarchy, and that all the data is initially in slow memory, that \(m\) is the number of memory elements (words) moved between fast and slow memory, \(t_m\) and \(t_f\) the slow and fast memory operation times, \(f\) the number of arithmetic operations, and \(q=f/m\) the average number of flops per slow memory access, or \gls{ai}. The minimum possible run time is \(ft_f\), which occurs when all the data can be held in fast memory. But, the actual time is:

\begin{equation}
\begin{aligned}
\textrm{actual time}=& ft_f+mt_m\\
=& ft_f\left(1+\frac{t_m}{t_f}\frac{1}{q}\right)\\
\end{aligned}
\end{equation}

\(t_m/t_f\) is a purely hardware characteristic, and is referred to as the machine balance. To get to half of the peak speed, \(q\geq t_m/t_f\), where the machine balance is typically in the range of 5 to 40. This analysis ignored, however, the fact that arithmetic operations can overlap memory operations - you wouldn't add the memory and flop times, you would instead take their maximum.

\begin{enumerate}
\item Two-loop matrix-vector multiplication (DGEMV):

\begin{lstlisting}[basicstyle=\ttfamily\small]
// read x into fast memory
// read y into fast memory
for (int i = 0; i < n; ++i)
{
	// read row i of A into fast memory
	for (int j = 0; j < n; ++j)
	{
		y(i) = y(i) + A(i, j) * x(j);
	}
// write y back to slow memory
}
\end{lstlisting}

Assuming that \(x\) and \(y\) can be read into fast memory, we have three read/write operations performed on \(x\) and \(y\), and \(n^2\) read operations for the matrix \(A\) (since it has \(n^2\) total values). So, we have \(3n+n^2\) slow memory references. We have one multiply and one add for each loop, and there are \(n^2\) total loop evaluations, so we have \(2n^2\) arithmetic operations. So, \(q=(3n+n^2)/2n^2\approx2\). \(q\) must be anywhere from 5 to 40 to achieve 50\% of peak speed for most machines. This simple analysis ignored the parallelism between memory and arithmetic in the processor - some analyses drop the arithmetic term entirely, since the memory and arithmetic usually occur in parallel, and only the maximum of these run times is the actual run time (and because the memory operations are so slow, it will almost always be memory-dominated). Matrix-vector multiplication tends to run right at the peak bandwidth of the machine, since the operation is so memory-dominated (not a lot of reuse of information).

\item Three-loop matrix-matrix multiplication (DGEMM): 

\begin{lstlisting}[basicstyle=\ttfamily\small]
for (int i = 0; i < n; ++i)
{
	// read row i of A into fast memory
	for (int j = 0; j < n; ++j)
	{	
		// read C(i, j) into fast memory
		// read column j of B into fast memory
		for (int k = 0; k < n; ++k)
		{
			C(i, j) = C(i, j) + A(i, k) * B(k, j);
		}
	}
// write C(i, j) back to slow memory
}
\end{lstlisting}

Because there is one multiplication and one add, which occurs in the \(n^3\) total loops, this algorithm has \(2n^3\) flops. Each column of \(B\) is read in to \(n^2\) loops, giving \(n^3\) memory operations, each row of \(A\) is read in once, giving \(n^2\) operations, and each element of \(C\) is read/written one time, giving \(2n^2\) operations. The total number of memory operations is therefore \(n^3+3n^2\). \(q=2n^3/(n^3+3n^2)\approx 2\), which does not give any advantage over matrix-vector multiply.

Matrix-matrix multiply has much greater algorithmic intensity than matrix-vector multiply because values are reused during the multiplication process, while values can only be used once during matrix-vector multiplication (matrix-vector multiplication necessarily has a lower \gls{ai}). Matrix-matrix multiplication is more limited by the peak performance than the bandwidth of the machine.

\item Blocked (tiled) matrix-multiply (DGEMM):

\begin{lstlisting}[basicstyle=\ttfamily\small]
for (int i = 0; i < n/N; ++i)
{
	for (int j = 0; j < n/N; ++j)
	{	
		// read block C(i, j) into fast memory
		for (int k = 0; k < n/N; ++k)
		{
			// read block A(i, k) into fast memory
			// read block B(k, j) into fast memory
			
			// itself contains 3 nested loops containing the matrix-multiplication
			C(i, j) = C(i, j) + A(i, k) * B(k, j); 
		}
	}
// write block C(i, j) back to slow memory
}
\end{lstlisting}

This is very similar to the previous algorithm, except that we only read in blocks of \(A, B, C\) into fast memory, performing matrix multiplies on the blocks. People have developed this algorithm both for cache tiling and register tiling (though register tiling will look somewhat different from cache tiling because you may just want to write the multiplication of the blocks by hand because they are so small - this is referred to as ``loop unrolling''). This takes advantage of temporal locality by repeatedly using the values of the matrices until they have all been used up, and only then reading in more information. 

Each block of \(C\) is read/written \(n^2\) times, giving \(2n^2\) memory operations. Each block of \(A\) and \(B\) are of size \(b^2\), where \(b\) is the block size. This will be read in \(N^3\) times, so that for each \(A\) and \(B\), the slow memory operations are \(Nn^2\). The total number of memory operations is therefore \((2N+2)n^2\), so the \gls{ai} is \(q=2n^3/((2N+2)n^2)\approx n/N\). So, we can improve the performance by increasing the block size. But, you cannot make the block sizes arbitrarily large, since all three matrices must fit in fast memory. If we have \(M_{fast}\) fast memory, then \(3b^2\leq M_{fast}\) for this tiling algorithm to be efficient. Rearranging, \(q\approx b\leq (M_{fast}/3)^{1/2}\) shows that the fast memory size limits the possible algorithmic intensity. To run at half peak speed, \(q=t_m/t_f\), so plugging this into the previous expression will give the approximate fast memory required. This required size is reasonable for the L1-cache, but not for the registers. The lower bound for matrix-matrix multiplication is given by \(q=(M_{fast}/3)^{1/2}\), so the computational intensity is bounded by the fast memory size. This lower bound also extends to anything similar enough to the three nested loop structure of matrix-matrix multiply.

Each of the matrix-multiplies contains three nested loops. There are three nested loops for each level of memory (including slow memory), where the block for each level of memory is assumed to fit (?). The matrix block sizes are machine-dependent. The blocks cannot be too large, or else they won't fit in the cache, but if they are too small, you algorithm loses \gls{ai}. For strange block sizes, if you have a direct-mapped cache, you may have more cache misses due to interference. Often, you select non-square block sizes to take advantage of row-major or column-major matrices, since reading in one direction will be must faster than in the other. The block sizes may not have symmetric behavior (2x3 behaves differently from 3x2) due to differences in row and column major. Too small of blocks is usually not an issue.

Note that the reason why we are seeking to run at half of peak speed is that we are not accounting for the fact that single processors have some parallelism in that they can perform arithmetic and memory operations at the same time. Assuming our program is split 50/50 between these two tasks, then we should shoot for 50\% peak speed. The lower the bandwidth of your machine, the higher the amount of fast memory that you need; vice vera, higher bandwidths allow you to have smaller fast memories. The blocked algorithm will give slightly different results from the three-nested-loop algorithm due to roundoff, but this is okay. But, for this reason, most compilers will not do this optimization for you due to changes in floating point arithmetic unless you pass in a high-enough optimization flag.

\item Because we need to minimize communication between \textit{all} levels of memory, the tiled algorithm may not be a good choice because it requires the selection of a good block size. Cache Oblivious Algorithms treat a matrix multiplication as a set of smaller problems that, if divided recursively, will eventually fit in the cache. These algorithms will asymptotically minimize the amount of data transferred between every level of the memory hierarchy, and are oblivious to the number and size of the levels.

\begin{lstlisting}[basicstyle=\ttfamily\small]
double func RMM(A, B, n)
{
	C11 = RMM(A11, B11, n/2) + RMM(A12, B21, n/2);
	C12 = RMM(A11, B12, n/2) + RMM(A12, B22, n/2);
	C21 = RMM(A21, B11, n/2) + RMM(A22, B21, n/2);
	C22 = RMM(A21, B12, n/2) + RMM(A22, B22, n/2);
}

C = RMM(A, B, n);
\end{lstlisting}

The required flops is 8 times the number of operations required for multiplication of matrices of size \(n/2\) plus \(4(n/2)^2\), the required number of additions of matrices of size \(n/2\). By a geometric series, the number of operations is \(2n^3\), which is the same as with the three nested loops. The number of slow memory operations is 8 times the data movement cost of each of the subproblems of size \(n/2\) plus \(4\cdot3(n/2)^2\) due to the four adds of the three matrices \(A, B, C\) that we assume we have to load into fast memory from slow memory for each loop (assuming none of the matrices can fit into fast memory - otherwise, this contribution would be \(3n^2\)). Once the three matrices fit into fast memory, you no longer have any memory operation costs. This algorithm does not require knowledge of the fast memory size.

In practice, you need to cut off the recursion at some point, because for small enough matrices, the function overhead becomes significant. You also need to very carefully implement the code in the micro-kernel, which is executed once you stop recursion. Prefetching is also necessary to compete with other codes. You also won't run at the fastest speed possible because the recursion cutoff point is likely smaller than the maximum possible cutoff point where you would perfectly utilize the fast memory size.

\item Z-Morton Ordering may be used to achieve a better matrix ordering in memory. Matrices are stored as a contiguous arrangement of entries, but in the tiled method we want to access blocks at a time, which will, depending on the cache line size, require us to jump around in memory (non-zero stride). Blocked orderings use different orderings for the matrix entries - each block is in contiguous memory locations. This improves spatial locality. These orderings are sometimes implemented by copying the matrix to a new, recursive layout, but leaving the normal layout in memory in another location so that indexing the values for other purposes (such as finding \(A(4,7)\)) is easy - this is referred to as a copy optimization. Copy optimization may not always be a time-saving algorithm.

\begin{figure}[H]
\centering
\includegraphics[width=0.25\linewidth]{figures/Z-morton-ordering.pdf}
\caption{Z-Morton Ordering of a matrix in memory.}
\end{figure}

\item Strassen's method is a matrix multiplication method that is \(O(n^{log_2(7)})\), as opposed to \(O(n^3)\) flops. Where matrix multiplication normally takes 8 multiplies and 4 additions, Strassen's method uses 7 multiplies and 18 adds, which extends to larger matrices by divide and conquer. Strassen's method is actually forbidden to be used on the benchmarks for the Top 500 computers, since the benchmarks are designed to test the computer speed, not algorithm choice. The point at which Strassen's method outperforms conventional matrix-matrix multiply is machine-dependent.

\end{enumerate}

It is possible to asymptotically achieve lower than \(O(n^3)\) operations, but aside from Strassen's method, these asymptotic methods requires excessively high matrix sizes to overcome the leading coefficients in the order-of-magnitude estimates. However, matrix multiplication can't get any better than \(O(n^2)\), since you must at least read in the matrices.

\subsection{Other Performance Tips}

\begin{itemize}
\item Remove false dependencies in the code. For two variables, the compiler will not know if they are pointing to the same location in memory. If two variables actually point to the same location, you don't want to read in both values. With Fortran, you can pass two variables into a function, which tells the compiler that neither of the variables is an alias to the other. In C, the restrict keyword can be used on pointers.
\item Try to get variables that you're going to use very often into registers, instead of the cache - the compiler may not do this automatically. For example, ahead of a loop that is going to repeatedly use some variables, assign those variables other names just outside the loop so that they are already stored in the registers before entering the loop. There is also a {\tt register} keyword in some languages.
\item Use loop unrolling, where you explicitly type out instructions instead of having extra language features such as loops that have their own overhead. This can be tricky, however, and you might need cleanup code to account for a loss of logical operations. You can also fill the instruction cache (holds the assembly language instructions) if you have too many instructions in your code. As opposed to the instruction cache, the data cache contains the L1, L2, etc. caches.
\item Reduce instruction latency by writing operations in the correct order to take advantage of \gls{ilp}.
\end{itemize}

\subsection{Basic Linear Algebra Subroutines}

The \gls{blas} is an industry standard for how to perform certain linear algebra actions in your code, which began in the 1970s because the compiler didn't always do a good job of optimizing code. In addition, \gls{blas} was used to ensure correctness, and to ensure that you don't exceed overflow or underflow by testing for these rare events. Further versions of \gls{blas} performed the operations with higher computational intensity to achieve faster run times. Then, all linear algebra libraries, such as LAPACK, used \gls{blas} to write their subroutines.

\section{Parallel Programming Models}

While there is not a one-to-one correspondence between the machine type and the programming model, there are general trends that work best. Historically, when people developed a new parallel machine, they also developed a new programming model. This contributed to the delay in accepting parallel programming due to the complications. During the development of programming models, it was important that the software be portable among different machines, but recently that has been a redux in that we are now developing machine-specific models because people are starting to use \gls{gpu}s. However, there are many standards that have been enacted to make sure \gls{mpi} can run correctly 10 years in the future.  

A parallel machine has multiple processors, multiple memory locations, and an interconnection network that connects everything together. The interconnection network, if simple, is sometimes referred to as a bus. A torus is a nearest-neighbor mesh (Hopper), where processors only communicate with those nearby. A dragonfly topology (Edison)...

\subsection{Shared Memory}

Shared memory is the most common parallel machine hardware, and also the easiest to program - most laptops are shared memory systems. A laptop contains multiple cores, with one shared memory. This parallel machine hardware is the most difficult to scale to large processors, and scaling becomes dramatically more difficult beyond 32 processors. A program is a collection of threads of control - each thread is essentially a processor. You can dynamically create more threads at runtime, and act as if you have more processors than there actually are. You begin by calling an initially-sequential program, and then initiate new threads. Each thread has a set of private variables (local stack variables). All threads access a shared memory, which includes static variables (the shared variables), shared common blocks, or the global heap. Threads communicate implicitly by writing and reading shared variables, and coordinate by synchronizing on shared variables. When a processor accesses something in shared memory, it makes a copy of that data into its own local memory before doing any actions with that data. Then, any writes to be made to the shared variable are done to the shared location. All computations take place in private registers.

A race condition, or data race, occurs when two threads (or two processors) access the same variable and at least one thread does a write. Because the action is performed on the register, which sits right next to the processor, two threads should not attempt to write to the same value at the same time because it is possible that the two writes would happen simultaneously. These accesses, if not controlled, are not synchronized, and at least one write will be lost, even though the hardware makes sure nothing catastrophic occurs if two threads try to write to the same location at the same time. The atomic operations are reads and writes, and \textit{not} algebraic operations such as additions. To avoid race conditions, put locks in the code ({\tt static lock lk;}) - only one thread can hold a lock at a time, which prevents simultaneous read/write operations. Most work should not be in the critical region (the locked region). Some libraries include locks hidden in their functions so that you don't have to worry about specifying them.

OpenMP is used to program shared memory at the CPU level. Each OpenMP thread corresponds to the same program being run on another processor on the same computer chip.

\subsubsection{Shared Memory}

The most common variant of the shared memory programming model is the multi-core chip model that dominates laptops. All the processors in a shared memory machine are connected to a large shared memory - these processors are typically called \gls{smp}s because there are multiple processors to a chip and they are ``symmetric'' in that each processor has somewhat of an equal role. Each chip has multiple cores, but all the caches are shared (but each processor also has its own cache). The bus is the interconnect, which acts like the lock that synchronizes the processors.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/shared-memory.pdf}
\caption{A shared memory machine.}
\end{figure}

Example vendors of this memory system include SGI, Sun, HP, Intel, AMD, and IBM. This machine has \gls{uma}, which means that the data shared by the processors does not take longer to access if not in the local cache for one processor than for another. The shared bus is the largest bottleneck, which is why this cannot scale to many processors. The hardware keeps the caches coherent, meaning that if one processor has outdated data, then cache protocol rules implement extra coherency traffic to make sure that any processor that needs data has the most up-to-date data. This coherency is not only a waiting bottleneck, but requires extra operations. Having a larger number of processors will not necessarily lead to a proportional improvement in overall runtime, since those processors, for large enough problems, will have to all access shared memory.

\subsubsection{Multithreaded Processor}

Each processor is not responsible for running a single subroutine, but rather, it can run many subroutines. A single processor may be assigned multiple threads to execute - when one thread reaches a load, that processor will switch over and begin executing one of its other threads, while the load occurs in the background in parallel. For multithreaded processors, there are a given number of threads that can be assigned to each core, so that the total number of things that could be happening at once is the number of threads per core multiplied by the number of cores. One function execution may be spread over multiple cores. This model is also difficult to scale up to many processors.

\subsubsection{Distributed Shared Memory}

Memory is logically shared, but physically separated. Any processor can access any address in memory. When accessing something not on its private cache, a processor will look in a table to determine in which memory location that data is stored. The information might be in its local memory or further away - cache lines, or pages, are passed around the machine. The cache line size or page size must be sufficiently large to amortize the overhead of the lookup table. This model has the same cache coherency problems as the shared memory model, and hence only scales up to 512 processors. 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/distributed-shared-memory.pdf}
\caption{A distributed shared memory machine.}
\end{figure}

\subsection{Message Passing}

All the computers on the Top 500 list are message passing machines. Every processor has its own memory, and processors communicate by sending messages to each other. This hardware format can scale to arbitrarily many processors. This is also one of the most portable ways of programming, since there are standards for sending and receiving data.

A program consists of a collection of named processes that are usually fixed at program startup time. Basically, one program is executed by all the processors. There is no shared data at all - each processor has its local data that is used for its own operations. Processes communicate by explicit send/receive pairs over a network. \gls{mpi} is the standard way for executing message passing. However, the continued use of \gls{mpi} may discourage innovation. Hybrid computers will program shared memory at the local level and \gls{mpi} at higher levels to allow faster operation.

A deadlock occurs when one processor attempts to send to another, but that processor never writes a receive statement, so the sending processor waits until its message is received. Avoiding deadlock becomes more difficult when you have more than two processors. 

\subsubsection{Distributed Memory}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/distributed-memory.pdf}
\caption{A distributed memory machine.}
\end{figure}

A distributed memory model has processors each with their own memory, and this machine model is the most common form of message passing. Then, the processors communicate through an interconnection network. Most of the Top 500 computers are distributed memory machines, but the nodes as \gls{smp}s. Each processor has its own memory and cache, but cannot directly access another processor's memory. The \gls{ni} chip allows a processor to access the memory of another processor, and controls the message passing for a processor. This separation of roles allows distributed memory to scale to arbitrarily large numbers of processors.

This machine model began development in 1994 by the simple connection of multiple computers to each other using ethernets. This PC cluster was called Beowulf, and was very cost effective since it simple combined off-the-shelf parts together. 

\subsubsection{Internet and Grid Computing}

The grid is a collection of computers connected over the internet. This is the largest parallel machine in the world. BOINC has 3.3 million hosts, with about 1000 CPU years per day. 

\subsection{Global Address Space}

This attempts to give the illusion of shared memory, even though this runs on a distributed memory machine. This is an intermediate point between shared memory and message passing. This allows scaling to arbitrarily many processors. A program consists of a collection of named threads, where each thread runs on a separate processor. There is both local and shared data, but shared data is partitioned over local processes. All the complexity of sends and receives are hidden from the programmer. This can make it difficult to think about locality if it is less obvious where the data is stored. Examples include \gls{upc}, Titanium, and Co-Array Fortran. 

\subsubsection{Global Address Space}

The global address space machine model can be run on any distributed memory machine that has \gls{ni} cards. The \gls{ni} do the communication between the processors - instead of loads/stores, you do puts/gets using one-sided communication. Processors are not interrupted by data transfer by the \gls{ni}s. There is still the potential for race conditions. 

\subsection{Data Parallel}

A single thread of control consists of parallel operations. This is easy to program because coordination is implicit - statements are executed synchronously. However, not all problems fit this model, and this is difficult to map onto coarse-grained machines.

\subsubsection{SIMD}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{figures/simd.pdf}
\caption{A \gls{simd} machine.}
\end{figure}

\gls{simd} systems consists of a large number of usually small processors. A single control processor issues each instruction, and each processor then executes the same instruction. Originally, these machines were specialized to scientific computing. This programming model can be implemented directly in the compiler, but can become difficult when there is more parallelism than processors available. 

\gls{simd} instructions use a single instruction to perform the same operation on multiple pieces of data, while scalar operations issue one instruction per operation. Unfortunately, \gls{simd} instructions can only be used for predefined processing patterns - \gls{simd} instructions cannot be used to process multiple data in different ways (such as an add alongside a subtraction). 

Data types used for \gls{simd} operations are called vector types. The length of a vector type is the length that fits into a register - 128 bits on \gls{sse} and 256 bits on \gls{avx}. So, double precision \gls{simd} operations provide the smallest degree of possible benefit from \gls{simd} instructions, since only four double precision values can be stored in a register at one time. To convert data to vector type, you need to cast a pointer to the scalar to a pointer to the vector type. 

Taking advantage of vectorization is essential to obtain fast programs. Compilers will automatically try to vectorize your loops, but the compiler may not be able to do so if you have loop dependencies, where you need previous values for future iterations. Conditional statements within loops will also prevent compiler vectorization. For Intel compilers, you can pass the flag {\tt -qopt-report=3 -qopt-report-phase=vec}, and for GCC compilers, {\tt -ftree-vectorize -fopt-info-vec-missed} to investigate which loops were not vectorized. You can also use pragmas, which are directives in C that allow you to pass additional information to the compiler.

In C/C++, the main thing that prevents vectorization is possible aliasing in pointers. Use the {\tt restrict} keyword for functions that accept multiple arrays or pointers. C++ doesn't have a {\tt restrict} keyword, but most compilers support {\tt \_\_restrict\_\_}. For instance, if you pass in two pointers to a function, where one pointer is pointing to the second element of the other pointer, then there is loop dependency, since any changes to a value of the first pointer also change the value of the second pointer. This prevents loop unrolling.

\subsubsubsection{SSE}

\gls{sse} had registers that were 128 bits each. 

\subsubsubsection{AVX}

Intel \gls{avx} is a set of instructions for doing \gls{simd} on Intel CPUs. The width of the \gls{simd} register, of which there are 16, was increased from 128 bits (legacy \gls{sse}) to 256 bits (32 bytes) in Intel \gls{avx}. An example of this type of architecture is the Intel Xeon Sandy-Bridge or Intel Ivy Bridge, which can perform four double precision operations concurrently. Some instructions take four-register operands. 

In addition, three-operand, nondestructive operations were added, so that an operation such as \(A=A+B\), which would overwrite \(A\) in the process, can now be written as \(C=A+B\), leaving the original operands unchanged. In addition, \gls{fma} was added. 

\subsubsubsection{Intel Xeon Phi}

The Intel Xeon Phi can perform 8 double precision operations concurrently. A 512-bit register is now typically the norm for new computer chips that are being developed.

\subsubsection{Vector}

Vector machines are based on a single processor, with multiple functional units that can perform the same arithmetic operation at the same time. Some processors can be turned on or off. Vector machines were overtaken by MPPs in the 1990s where you could connect many off-the-shelf processors, but they are re-emerging in recent years. 

\subsubsection{GPU}

GPUs are essentially a version of vector architecture. The key idea is that the compiler does some of the work of finding parallelism so that the hardware doesn't have to. 

Companies originally developed their own programming languages - Nvidia uses CUDA for programming on GPUs. While \gls{mpi} is now the standard for programming on distributed memory machines, the GPU community is now selecting a standard for programming with GPUs. OpenCL is the standard for programming with GPUs. 

\subsection{Hybrid}

The largest machines on the Top 500 list are actually clusters of the machines listed previously - they are heterogeneous. 14\% of the Top 500 list have accelerators, and those computers accounted for 35\% of the performance. Multicore/\gls{smp}s are a building block for a larger machine with a network. Each node is a multicore chip with many processors. The old name for this type of hybrid machine was \gls{clump}. The simplest way to program this type of machine is to treat the machine as ``flat,'' and use one \gls{mpi} process per core. Using \gls{mpi} to communicate between two processors on the same chip is a waste of resources, however, so shared memory programming should be done within on \gls{smp}, but message passing outside the \gls{smp}. One core will be dedicated to performing \gls{mpi} with all the other processors on the machine. 

DARPA was involved in a project to develop a programming language to hide the duality between shared memory programming at a \gls{smp} but \gls{mpi} between processors, though this has not yet taken off. Global address space models can often call message passing libraries and vice versa. 

Within a single node, run OpenMP, and between nodes, run \gls{mpi}. It appears that the best performance is obtained by running two or four \gls{mpi} processes per node. OpenMP does not scale well to high numbers of cores due to the fork/join actions that create bottlenecks. It is generally better to reduce the number of \gls{mpi} processes in favor of increasing more OpenMP processes, but you want to obtain a balance between the two.

\subsection{Cloud Computing}

Cloud computing allows people to easily shared thousands of computers. You pay a fee to access these resources.

\section{Sources of Parallelism and Locality in Simulation}

There is often a great deal of independence between objects, and objects tend to depend much more on nearby objects than on distant objects. Dependence on distant objects can also often be simplified. In addition, when a continuous domain is discretized, time dependencies are generally limited to adjacent time steps. 

\subsection{Discrete Event Systems}

The objects, state, time, etc. are all discrete. The set of all variables at a given time is called the state. Each variable is updated by computing a transition function depending on the other variables. These systems can be synchronous or asynchronous. For synchronous systems, all transition functions are evaluated at each time step - this is also called a state machine. At each time step, the old state is read, and the new state is written, so there is no possibility for a race condition. This doubles the required memory, however. 

The asynchronous system only evaluates the transition function if the inputs have changed - this is also referred to as event-driven simulation. This is much cheaper than always looking to your neighbors to see if something has changed from the previous time steps. Every event is associated with a time stamp, but there is no global time step. Asynchronous simulation is much more efficient, but more difficult to parallelize because it's more difficult to decide when to ``receive.'' Conservative asynchronous simulations only simulate up to and including the minimum time stamp of the inputs. However, then you need deadlock detection if there are cycles in the graph. Speculative asynchronous simulations, on the other hand, assume that no new inputs will arrive, and keep simulating. You may need to backup if this assumption is wrong. 

Locality is achieved using domain decomposition, where the domain should be split up so as to minimize the amount of communication required between pieces of the domain. Basically, you want to minimize the surface to volume ratio. Graph theory is used to find the optimal domain decomposition, and graph partitioning is the process of assigning subgraphs to the different processors. You might not simultaneously be able to achieve load balancing with minimal communication between processors. Use a quad-tree (2-D division by recursive rectangle division) or an oct-tree (3-D) to initially obtain a good balance between load balancing and minimal communication. This is an NP-hard algorithm, which means that the optimal algorithm requires an exponential number of processors. Domain decomposition is more complicated for particle systems if there are far-field forces, however, since every particle still impacts every other particle - this is usually accounted for by passing around each processor's particle to all the others. 

\subsection{Particle Systems}

Particle systems are essentially a version of lumped systems. Time is now continuous, but your system is largely discrete because you have a finite number of objects. Forces on a particle can be divided into the external force, the nearby force, and the far-field force. External forces can be computed regardless of the other particles, and are embarrassingly parallel. Far-field forces can often be approximated as simpler forms; far-field forces are typically governed by elliptic PDEs. Nearby forces are the most difficult to parallelize. In addition, for particle systems, it is very easy to distribute an even number of particles amongst the processors to achieve good load balancing and locality.

Parallelizing nearby forces scales as \(n^2\) if you need to look at all pairs of particles to determine the nearby forces - domain decomposition allows this to be simplified. The ``ghost zone'' is the boundary region between all separate domains. 

\subsubsection{Particle Mesh Methods}

Superimpose a mesh on the particles, and move the particles to the nearest grid point. Far field forces are easy to solve on a regular mesh, so the complexity drops to \(O(nlog(n)\) instead of \(n^2\). This method is widely-used.

\subsubsection{Tree Decomposition}

Forces from a group of far-away particles are simplified to a single ``large'' particle. Each node on the tree contains an approximation of its descendants. This method also scales as \(nlog(n)\) instead of \(n^2\) for approximating far-field forces. 

\subsection{ODEs}

Entities are discrete, but time is continuous - ODEs represent lumped systems, because space is not continuous. 

\subsection{PDEs}

Everything is continuous - both space and time. Elliptic equations are generally steady-state, with global space dependence. Hyperbolic equations are generally time-dependent with local space dependence (finite wave speeds limit communication with distant regions of the domain). Parabolic equation are generally time-dependent with global spatial dependence. In order of increasing difficulty of parallelization, these equations ranks 1) elliptic, 2) hyperbolic, and 3) parabolic. Global dependence results in either a lot of communication or very small time steps. From a numerical stability point of view, hyperbolic equations are the most difficult to solve. 

Explicit methods only require a matrix-vector multiplication, while implicit methods require a matrix solve at each time step. Iterative solvers convert this required matrix solve into matrix-vector multiplications.

\subsection{SpMV}

\gls{spmv} is most commonly performed in a \gls{csr} format. You only want to store and do arithmetic on nonzero entries. All the data is stored in one array, where only the nonzero values are stored. Then, you have two additional arrays - one holding pointers to the rows, and one holding pointers to the columns. The column vector holds pointers that indicate which positions in each row (i.e. the column numbers) are nonzero. Each entry in the row vector indicates the position in the data vector that is the first nonzero value in that row. This can give a lot of cache misses in the resultant vector if the nonzero values are not clustered along the diagonal (the ideal structure for parallelization is a block diagonal matrix). There are algorithms to cluster the nonzeros together to avoid this cache miss problem. A potential issue with parallelization is that all the processors might have to access every entry in the vector \(x\). Instead of breaking up the rows into blocks, you could split them up however you want for the processors to minimize the accesses to \(x\). Locality also improves when there are few nonzeros off the diagonal. The reason that you have communication with respect to accessing \(x\), you generally want to avoid copying data to each processor because this will eat up your memory.

Applying \gls{ge} or cholesky decomposition will fill in a matrix with zeros. A matrix can be reordered to reduce this fill-in if you intend to use \gls{ge}.

\subsection{Graph Partitioning}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{figures/graph-partitioning.pdf}
\caption{An illustration of the relationship between graph partitioning and a matrix.}
\end{figure}

A sparse matrix and a graph are different representations of the same concept - edges in the graph are nonzero in the matrix. The edges of the graph become off-diagonal terms.


I am adding a line to figure out precisely what git fetch does.
\end{flushleft}
\end{document}
