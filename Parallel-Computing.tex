\documentclass[10pt]{article}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{setspace}
\usepackage{ragged2e}
\usepackage{color}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{array}
\usepackage{makecell}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xfrac}
\usepackage{etoolbox}
\usepackage{cite}
\usepackage{url}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage{courier}
\usepackage{url}
\usepackage{esvect}
\usepackage{commath}
\usepackage{verbatim} % for block comments
\usepackage{enumitem}
\usepackage{hyperref} % for clickable table of contents
\usepackage{braket}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{gensymb}
\usepackage{longtable}
\usepackage{soul} % for striking out text
\usepackage{tcolorbox} % for colored boxes
\tcbuselibrary{breakable} % to allow colored boxed to extend over multiple pages
\usepackage[makeroom]{cancel}	% to cancel out text
\usepackage{breqn}
\usepackage[mathscr]{euscript}
\usepackage{listings}
\lstset{
	basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}

% for circled numbers
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}


\titleclass{\subsubsubsection}{straight}[\subsection]

% define new command for triple sub sections
\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\newcommand{\volume}{\mathop{\ooalign{\hfil$V$\hfil\cr\kern0.08em--\hfil\cr}}\nolimits}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

% Generate the glossary of acronyms
\usepackage[acronym]{glossaries}
\makeglossaries

\newacronym{ai}{AI}{Arithmetic Intensity}
\newacronym{alu}{ALU}{Arithmetic Logic Unit}
\newacronym{api}{API}{Application Programming Interface}
\newacronym{avx}{AVX}{Advanced Vector Extensions}
\newacronym{blas}{BLAS}{Basic Linear Algebra Subroutines}
\newacronym{clump}{CLUMP}{Cluster of Symmetric Multiprocessor}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{csr}{CSR}{Compressed Sparse Row}
\newacronym{cu}{CU}{Control Unit}
\newacronym{dram}{DRAM}{Dynamic Random Access Memory}
\newacronym{fma}{FMA}{Fused Multiply-Add}
\newacronym{ftmpi}{FTMPI}{Fault Tolerant MPI}
\newacronym{ge}{GE}{Gaussian Elimination}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{ht}{HT}{Hyper Threading}
\newacronym{ilp}{ILP}{Instruction Level Programming}
\newacronym{icv}{ICV}{Internal Control Variable}
\newacronym{lifo}{LIFO}{Last In, First Out}
\newacronym{mpi}{MPI}{Message Passing Interface}
\newacronym{mpp}{MPP}{Massively Parallel Machine}
\newacronym{mr}{MR}{Map Reduce}
\newacronym{ni}{NI}{Network Interface}
\newacronym{numa}{NUMA}{Non-Uniform Memory Access}
\newacronym{openmp}{OpenMP}{Open Multi-Processing}
\newacronym{pci}{PCI}{Peripheral Component Interconnect}
\newacronym{pgas}{PGAS}{Partitioned Global Address Space}
\newacronym{pod}{POD}{Plain Old Data}
\newacronym{posix}{POSIX}{Portable Operating System Interface}
\newacronym{pvm}{PVM}{Parallel Virtual Machine}
\newacronym{pram}{PRAM}{Parallel Random Access Memory}
\newacronym{rdd}{RDD}{Resilient Distributed Dataset}
\newacronym{simd}{SIMD}{Single Instruction, Multiple Data}
\newacronym{smp}{SMP}{Symmetric Multiprocessor}
\newacronym{spmd}{SPMD}{Single Program, Multiple Data}
\newacronym{spmv}{SpMV}{Sparse Matrix-Vector Multiply}
\newacronym{sram}{SRAM}{Second Random Access Memory}
\newacronym{sse}{SSE}{Streaming SIMD Extensions}
\newacronym{tlb}{TLB}{Translation Look-Aside Buffer}
\newacronym{uma}{UMA}{Uniform Memory Access}
\newacronym{upc}{UPC}{Unified Parallel C}

\begin{document}

\begin{centering}
\Large CS 267: Applications of Parallel Computing\\
\end{centering}

\tableofcontents
\clearpage

\section{Introduction}
\begin{flushleft}\justify

This document contains my course notes for CS 267, as well as my notes on general topics such as computer architecture.

\section{Common Terminology}

\subsection{Basic Computer Architecture}

\subsubsection{Processors and Cores}
A processor (synonyms: computer chip) is the ``brain'' of the computer. There are two basic components in a (single-core) processor:

\begin{enumerate}
\item The \gls{alu} performs mathematical, logical, and decision operations
\item The \gls{cpu} directs the operations handled by the processor
\end{enumerate}

A processor may have one or more cores - cores are the parts of the processor that receive instructions, work on those instructions, and return results. A core can run a single program, or multiple - they are the parts of a processor that do simultaneous processing of multiple tasks at a given time. The number of cores are essentially the number of operations that can be performed simultaneously by one processor, since each core has its own \gls{alu} and \gls{cpu}. Each core is itself technically a processor, since early computers has single-core processors, but the chip is manufactured in such a way that the different cores work in coordination. For most multicore chips, the cores each have their own L1 and L2 cache, but share a large L3 cache. Many cores per processor is not necessarily advantageous, since it requires the programmer to write algorithms with a very high degree of parallelism.

\subsubsection{Threads}
A thread is one (or more) hardware contexts within a single core. \gls{ht} is the process by which one core can run more than one thread, where those threads then share resources such as memory and arithmetic operation abilities within that core. Without \gls{ht}, you would only be able to have one thread per core, and the default with \gls{openmp} is to run one thread per physical core. So, with \gls{ht}, one physical core appears as multiple cores, allowing scheduling of multiple processes per core. The number of \gls{openmp} threads (per \gls{mpi} task) is specified with the {\tt -c} option.

\subsubsection{Nodes}
On the motherboard of the computer are one or more sockets. Each socket holds one processor. A node combines several sockets, such that a node contains several processors. Edison nodes contain two processors with 12 cores each. Some memory is shared between the sockets on a node, so memory is shared not only between cores on a single processor, but also between processors on a single node. Two-socket nodes are very standard, and you can also buy four-socket nodes, but improvement diminishes beyond that due to \gls{numa} issues. 

A cluster is a bunch of nodes wired together using explicit networking. Nodes do not directly share memory with each other - you must pass information around on the network. So, \gls{mpi} is required to utilize multiple nodes, but \gls{mpi} has even been used to program shared memory systems, despite the fact that all cores on a node share some amount of memory with each other. The number of nodes requested for a job are indicated with the {\tt -N} sbatch option. You can specify a minimum and maximum number of nodes to allocate, but if you provide only one number, then you will use exactly that many nodes.

\begin{lstlisting}
#SBATCH -N 24
\end{lstlisting}

\subsubsection{Tasks}
A task is a single instance of a serial application or an \gls{mpi} application - this represents the number of times that the executable for your program is launched. If using \gls{mpi} to program in a distributed memory system, setting the number of tasks equal to the number of nodes will then run one \gls{mpi} task per node. However, more than one task can be run on each node - in that case, \gls{mpi} is used in something of a shared memory programming model by passing messages between the different cores in a node and between cores in different nodes. The \textit{total} number of tasks for your program are specified using the {\tt -n} option, where by default one task is specified per node; {\tt -n} is the multiplication of the number of tasks per node by the number of compute nodes indicated by the {\tt SBATCH} directive. Alternatively (?), you can specify the number of \gls{mpi} tasks per compute node using the {\tt -N} srun option. Edison works best for 1-4 \gls{mpi} tasks per node. 


A flop is a floating point operation, which is usually in double precision unless otherwise noted. While the fastest computer has a clock rate of 55 Pflop/s, most laptops are in the Gflop/s range. 

While there are a certain number of physical cores per CPU, \textit{virtual cores} refer to the multiplication of the number of physical cores with the number of CPUs on a node? 

Leadership machines are computers that are designed to get the best performance possible, though they might not be as user-friendly.

Threading occurs when you have more than one \gls{mpi} task per core. 

A stiff matrix has large differences between the largest and smallest entries, which means that a vector that is acted upon by \(A\) will change rapidly, which makes explicit methods less accurate (cannot capture sharp gradients as well). 

Strong scaling can be measured by keeping the problem size the same, but varying the number of processors, and hence reflects how much of your runtime is associated with the overhead of parallelism. Weak scaling, on the other hand, increases the problem size and number of processors proportional to one another; this shows you the relative amount of your runtime that is associated with parallelism overhead vs. the actual problem work.

DAXPY is a shorter name for double-precision \(a*x+y\). 

A scan is a parallel prefix operation that takes an original array, and performs a binary operation (such as add) among all of its elements - the first entry would be \(a_1\), the second \(a_1+a_2\), etc. 

Cores typically use one OpenMP thread per physical compute core, but \gls{ht} is possible, which allows two threads per physical compute core.   Using the maximum number of threads per node is not always ideal, however, and Edison performs best with 12-13 OpenMP threads per processor. At a maximum, you could have, on a single node, the number of \gls{mpi} tasks equalling the number of cores.

Each core has one or two (hyperthreading) threads and a 256 bit vector unit. 

A process is an instance of a computer program that is being executed, and consists of the program code and its current activity. A process can be made up of multiple threads of execution that execute instructions concurrently. Process-based multitasking allows you to do multiple things on your computer at the same time, such as using a text editor while compiling code. Each process has a complete set of its own variables. Threads are essentially processes that run in the same memory context because they can share data during execution. 

\subsection{The Stack and Heap}

The stack is a region of memory that stores temporary variables created by each function in a program. The stack is a \gls{lifo} data structure that is managed by the \gls{cpu}. Every time a function declares a new variable, it is pushed onto the stack. Then, when that function exits, all of the variables pushed onto the stack by that function are freed. Because stack variables are lost when a function exits, those variables are inherently local. Using the stack to store variables allows you to not have to worry about allocating memory by hand, and hence reading/writing to stack variables is very fast. There is a limit on the size of variables that can be stored on the stack. The {\tt static} keyword can be used to keep a variable on the stack around longer than it would have been otherwise (it won't be freed upon exit of the function). A {\tt static} variable is known globally only within a single file, while an {\tt extern} variable is global to all files.

Variables allocated to the heap, on the other hand, do not have such memory limitations (the limitations are those of the entire computer). The heap is a memory region that is not managed automatically for you - to allocate memory on the heap, use {\tt malloc}. You must deallocate memory once you don't need it anymore, and if you fail to do so, you have created a memory leak, which can be detected with a tool such as {\tt valgrind}. Reading and writing to the heap is slower than reading from the stack because you need to use pointers to access memory on the heap. Variables created on the heap are accessible by any function, and are basically global in scope. Both the stack and the heap are conceptual terms - there is no physical region of memory that is the ``stack'' or the ``heap.''

\subsection{Benchmarks}

All of the fastest computers in the world are evaluated based on their performance with regard to several benchmarks, most famous of which is the Linpack benchmark. This benchmark solves a dense \(Ax=b\) multiply, and reports peak speeds. While this benchmark is fairly representative of overall performance, it should be noted that a computer's peak speed is faster than the speed at which the Linpack benchmark is completed, sometimes as much as two times faster. The Gordon Bell prize recognizes that some achievement should be given to fastest speed, and awards the fastest Pflop/s speed of a machine on \textit{any} task.

The peak speed on the largest computers is currently around 100 Pflop/s, and the total computing speed of the top 500 list is approaching an Exaflop/s speed. The fastest computer accounts for roughly 10\% of the sum of the top 500. The average system age on the top 500 list has been increasing in the past several years, which shows that the performance is starting to slow in the industry.

\section{A Motivation for Parallel Computing}

Up until around 2005, many did not believe that parallel computing would succeed, and many of the early startups attempting parallel computing failed due to Moore's law. Moore's law, hypothesized by one of the cofounders of Intel, predicted that the transistor density on a computer chip would double every 1.5 years as the transistor size shrinks. Shrinking a transistor size by a factor of \(x\) results in a factor of \(x\) increase in the clock speed since the wires connecting the transistors are shorter. In addition, due to manufacturing improvements, the die size, or the size of silicon that is allocated to each computer chip, has also increased by a factor of \(x\). Decreasing the size of the transistor then also leads to an increase in the number of transistors per unit area by a factor of \(x^2\). Overall, this leads to an \(x^4\) improvement in computational speed every 1.5 years without any effort required for modification of a serial program. The actual performance observed improved by a factor of \(x^3\), however. This improvement was realized by 1) improving the on-chip parallelism with \gls{ilp}, where parallelism is implemented on each chip itself, and 2) by increasing locality by building caches, which reduces data movement. More transistors on a chip, however, does not necessarily lead to faster processors. We wanted to take advantage of having more transistors on a chip, so we added speculation into the processor logic - the program would guess where the execution would go next and would then go preemptively execute a header somewhere. In some cases, this wastes power.

However, this improvement in computational speed of serial programs could not continue forever due to several important limitations.

\begin{itemize}
\item Manufacturing limits - only with extremely high probability would all 8 processors on an 8-core chip function correctly - some chips were sold with 7/8 processors guaranteed to function
\item Cost to build a chip increases exponentially
\item Power density continues to increase, which makes the chips difficult to cool. Power scales as \(V^2fC\), where \(V\) is the voltage, \(f\) the frequency, and \(C\) the number of processors. Power increases as \(x^4\) with Moore's law, and hence the power density had to be reduced by lowering the clock speed.
\end{itemize}

Experts predict that Moore's law will continue through the early 2020s, but will then level out. Improvements in the clock speed have already begun to level out. So, the accommodate demands for improved computing, the number of cores on a computer chip has begun to increase. So, instead of getting faster processors in terms of frequency, we're getting more cores, which requires programming to be done in a parallel setting. Today, all major computer vendors produce multicore chips, and it's really not even possible to buy a sequential computer anymore.

\subsection{Challenges to Parallel Computing}

There are several key challenges that face the ability of parallel computing to speed up our computations. First, there must be enough parallelism in the task at hand. Some tasks in a program will be inherently serial, and those may turn out to be severe bottlenecks. Amdahl's law predicts the speedup, or reduction in runtime when using \(P\) processors instead of 1 processor, for \(s\) the fraction of the program that must be run in serial.

\begin{equation}
\textrm{Speedup}=\frac{\textrm{Time(1)}}{\textrm{Time(P)}}\leq\frac{1}{s/1+(1-s)/P}\leq\frac{1}{s}
\end{equation}

Second, the degree of parallelism must be sufficiently granular to amortize the cost of parallel programming. Moving data costs significantly more than arithmetic, so the chunks of parallel actions must be sufficiently large. However, the parallel chunks should not be too large that there is not enough parallel work. Parallel programming incurs overhead due to:

\begin{itemize}
\item Cost of starting a thread or process
\item 
\end{itemize}

\section{Single Processor Machines}
 
Even if you're running on a single processor, chances are you're not going to be operating at the peak performance speed, or the guaranteed-to-not-exceed speed advertised by the manufacturer and computed as the multiplication of the flops/cycle with the clock speed and number of nodes. Most applications run at around 10\% of the peak performance (flops/cycle * clock rate * processors). So, before parallelizing code, you should first try to run as fast as possible on one processor before splitting up operations among multiple processors. Most of this lost performance (both time and energy) is due to interacting with the memory system, moving data between different locations. 

The compiler will try to generate optimal code based on your hardware, but in practice the compiler does not know what is the best way to optimize. 

\subsection{Costs in Modern Processors}

Processors name bytes and words in its address space. These represents integers, floats, arrays, etc. Operations include read/write on this information, arithmetic, and logical operations. Each of the arithmetic and logical operations has \textit{roughly} the same cost, but the read and write operations are much more expensive. The compiler translates the human-typed code into assembly language that is understood by the computer (the code that is run on the processor), which adds in the read/write commands that are not explicitly written in human-typed code. For instance, to use a variable in a program, I simply type the name of that variable - I don't write the instructions to go find it in memory and then read it. Likewise, I use equal signs to write values. You could write assembly language to influence the registers, but this is typically done by the compiler. The compiler performs register allocation, which is the process by which information read from memory is written to a physical data structure called a register. All information to be used by the processor must first be loaded into a register. Registers are physical quantities that have very small, very fast memory to hold variables that are actually being used by the processor; they are located adjacent to the processor, on the computer chip. Then, arithmetic and logical operations are performed on the registers, where output results are written to registers (?) before being moved to other locations in memory if needed. By tailoring your program, you can encourage the compiler to act in a certain way with the memory system.

Processors also have caches, which are small amounts of fast memory that are physically located close to the processor on the computer chip, or off the chip. The caches are controlled by hardware, not by software (?). The hardware moves information into the caches, which can dramatically impact read operations. The cache is used to store values of recently used or nearby data that will not fit into the registers. When you actually need data, you move it from memory to cache, and then from cache to register. 

\subsection{Memory Hierarchies}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{figures/memory-heirarchy.pdf}
\caption{Memory hierarchy in a single processor.}
\end{figure}

\begin{itemize}
\item The \textbf{processor} is the part of the computer that performs actions. 
	\begin{itemize}
	\item Directly on the processor are \textbf{registers}, which are used when presently performing operations on variables. These registers are typically 16 bytes. The cost of register access is essentially zero.
	\item The processor typically also has an on-chip cache, referred to as the L1-cache (level-1 cache), or simply the cache. The size of the L1-cache is on the order of 16 kB, and is limited in size because it has to fit on the chip. The cost of L1-cache access takes about 1-2 cycles.
	\end{itemize}
\item Off the chip is the \gls{sram}, or other caches that are typically referred to as the L2-cache (\(\sim\)2 MB), L3-cache, etc., though some of these caches could also be on the chip. A large cache will always have delays, since hardware has to check longer addresses and due to associativity rules. More generally, the further from the chip, the greater the time delays due to the transit times through longer wires. The Cray T3E eliminated one cache to speed up misses of the cache. When a cache miss occurs, you waste time moving through the memory hierarchy to get to main memory. If you have a program that is well-optimized to the caches closer to the processor, then you don't need some of the further-away caches. IBM uses a ``victim cache,'' so that data in the cache that is about to be thrown away due to new incoming data that is to be stored in the same cache location can be stored in the victim cache. This is overall cheaper than sending back to \gls{dram} again.
\item \gls{dram}
\item Disk. Supercomputers don't have a disk attached, so they do not have any virtual memory (?). Clouds, on the other hand, do have virtual memory.
\item Supercomputers have tertiary storage, where robots move \textbf{disks/tape} into computers for reading.
\end{itemize}

Historically, while the CPU has been improving according to Moore's law, the DRAM latency (time for data to move off-chip and onto DRAM) has been improving at a much slower rate, so these memory hierarchies are getting deeper. We want to develop algorithms that touch memory as infrequently as possible. 

Cache is fast (expensive) memory that keeps a \textit{copy} of data, and the information about where it came from, in main memory; it is hidden from software. For data at memory address xxxxx1101, it will be stored at cache location 1101. So, every address location that ends with 1101 will be mapped to this same cache location (for a direct-mapped cache). When data is read in from memory, it is moved into the cache so that the next time you access that value it is faster. A cache hit occurs when, upon requesting a value for a register, that value is already stored in the cache. Alternatively, a cache miss occurs when that data is not already in the cache, and you have to go out further in memory to obtain it. Then, the hardware will store the requested data in cache and eliminate whatever was previously in cache to avoid such a miss in the future. With parallel programming, taking advantage of the fact that data is moved into the cache may be more difficult if the data is needed on every processor, since then the data movement which would only have to be performed one time has to be performed once for each processor. But, sometimes it is still worth it for processors to perform duplicate actions so that they don't need to communicate information.

The cache line length is the number of bytes loaded together in one entry - caches always load more than one value at a time, even though you may only request one line; this takes advantage of spatial locality. Loading in a single value will load in its \(n\) neighbors for a cache line length of \(n\). A direct-mapped cache can store only one address in a given range of cache (all xxxxx1101 values would be stored in cache location 1101). Because this can lead to bad behavior, associative caches, or \(n\)-way caches, which can store \(n\) of the xxxxx1101 locations at cache location 1101. Up to 16 xxxxx1101 entries can be stored in an associative cache. Associative caches reduce collision issues.

Also in the memory hierarchy are several other levels, such as virtual memory. Even if your program will not fit in main memory, some of the program will be put out on disk by providing extra address on the disk, where the hardware system maps those address. The \gls{tlb} determines whether information is held on disk or in main memory. A page is the amount of memory that can be read from disk to main memory in a group, and is 8 kB, and could be determined from a memory benchmark by looking for the plateau on an array size that can be read in to the L2-cache in entirety.

Latency is the time to transfer a single piece of data, and is typically denoted as \(\alpha\). The bandwidth is the speed of data transfer (bytes/second), and because the bandwidth is improving faster than the latency (23\% vs. 7\% per year), we want to use spatial locality and temporal locality to avoid unnecessary data transfer. The inverse bandwidth is typically denoted as \(\beta\), and is the more commonly-used bandwidth metric because it has the same units as latency. Both latency and bandwidth are hardware parameters, and we prefer to be limited by bandwidth instead of latency. We can handle these two aspects of slow memory operations, through the following concepts.

\begin{itemize}
\item Spatial locality is the desire to reduce the need to transfer data by reading in a chunk of data and using it all at the same time so that multiple read/use statements are not required.
\item Temporal locality attempts to reuse an item that was previously accessed so that you don't have to repeatedly access the same value and waste read time. Each time you read a value from memory, it is saved in registers or cache, eliminating another slow memory operation the next time you need it provided it has not been overwritten on the cache - this is known as bandwidth filtering.
\item We can take advantage of the fact that bandwidth is better than latency by allowing the processor to issue multiple reads/writes with a single instruction through vector operations.
\item We can take advantage of better bandwidth by using prefetching, which is the process by which the hardware guesses what you are going to require next. Then, the hardware goes and fetches that information ahead of time, whether or not you actually want to use it next.
\item We can issue writes, and then buffer them, so that they are sent to slow memory at a later stage. Both prefetching and write buffering require that nothing dependent is happening.
\item Because memory is run by separate hardware, you can run a bunch of memory instructions that can be run in parallel while the processor is doing arithmetic. So, the runtime is then the maximum of the arithmetic time and the memory retrieval time.
\end{itemize}

Supercomputers tend to have quieter environments (less background activity) than a laptop, so benchmarks will give much steadier runtime results. 

\subsubsection{Memory Benchmarks}

Data is read in from memory with varying strides and for varying array sizes, the smaller the stride the better, since cache lines larger than or equal to the stride will take advantage of data already read in from the previous stride. If the entire length of the data array fits in the L1-cache, then you will never experience the increase in cost of the further-away caches, and the stride has no impact on the retrieval cost, because when you load the data in once it stays in cache and you don't need to read it in any more times, and the total cost becomes equivalent to the cache hit time (time to access the L1-cache).

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{figures/membench.pdf}
\caption{Memory benchmark results, for a computer with only one level of cache.}
\end{figure}

If the data array does not fit in the L1-cache, then a plateau is reached that signifies the memory cost that is incurred every time there is a cache miss. The cost increases with stride initially because you will be able to reuse some data due to the cache line length, but for large enough strides, you don't get to reuse any of this information. The left corner on the plateau occurs at the cache line length. The right corner on the plateau occurs because the data may again be able to fit in the L1-cache (?).

A second memory benchmark involves the Stanza Triad, which attempts to evaluate the efficiency of prefetching. Read a bunch of consecutive memory locations, then skip a bunch, and then read consecutive again, and so on. The highest bandwidth would occur if we read in consecutive memory locations. The larger the stanza length, the closer we get to this optimal performance. Prefetching gets the next cache line (pipelining) while using the current one. This does not kick in immediately, but instead the performance depends on the length of data being read in. Prefetching will improve your performance if you're using the information just past the cache line you just loaded. Prefetching essentially increases the cache line size, if used successfully.

\subsection{The Roofline Model}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/roofline.pdf}
\caption{The roofline model.}
\end{figure}

The \gls{ai} is the total number of flops/total \gls{dram} bytes moved. Because memory operations are slow, the \gls{ai} should be large for high performance - because this is an algorithmic characteristic, we can tailor this. The goal is to attain as high as possible Gflop/s. \gls{ilp} \gls{simd}, and \gls{fma}, for instance, will raise the highest attainable Gflop/s (up to the peak performance). 

The diagonal line of the plot is determined by the peak \gls{dram} bandwidth. Lack of proper prefetching, for instance, will lower the attainable bandwidth. You want both a high bandwidth and a high \gls{ai}. There is no point trying to improve the arithmetic performance if you are bandwidth-limited.

\subsection{Parallelism within Single Processors}

The parallelism within a single processor is hidden from software. The hardware will automatically try to take advantage of parallelism within a single processor using pipelining and \gls{simd}. Depending on how you write your code, the compiler may or may not optimize correctly using these single-processor parallelism techniques. Compilers have optimization flags that tell the compiler to optimize in certain ways. 

Autotuning is the process by which you simply test every code configuration to figure out which one works best for your application - this was first used in CS 267. ATLAS, a matrix-matrix multiply tuner, is used in Matlab. For some algorithms, autotuning can be performed off-line, instead of at runtime. For example, for matrix multiplication, all that matters is the size of the matrix - the actual entries of the matrix don't have an impact, since a multiplication is always the same cost regardless of the numeric values themselves. Autotuning performs about as well as hand-optimized code.

\subsubsection{Pipelining}

If you have enough stages in an operation, then you can do many things at once. For example, to do several loads of laundry, its best to run the dryer while running the washer for the next load. Pipelining helps bandwidth, but not latency (the time to do a single load of laundry still takes the same length of time). Bandwidth is limited by the slowest part of the pipeline, and the potential speedup is the number of pipe stages. Pipelining is also used within arithmetic units. 

\subsubsection{Single Instruction, Multiple Data}

\gls{simd} is the process by which a single instruction is used to perform an operation on multiple registers at the same time. Using bigger registers, you can load multiple values into these registers and perform a single instruction to act on the multiple values in the register. The compiler will try to find \gls{simd} instructions and use them as much as possible. Because a register contains 16 bytes, \gls{simd} behaves differently if you're using different data types - because doubles are 8 bytes, while floats are 4 bytes, you can only achieve 2x parallelism with doubles, but 4x parallelism with floats. The challenge with \gls{simd} is that the data must be in contiguous memory locations when loaded - this requires a good degree of spatial locality. The instructions also have to move data around from one part of the register to another. With \gls{gpu}s, you can be performing single instructions with thousands of data pieces at the same time, a principle that is similar to \gls{simd} instructions. 

\subsubsection{Fused Multiply-Add}

Single processors also typically have a special operation called a \gls{fma} to perform the multiplication of two numbers and add it to a third in a single instruction. This can be done at the same bandwidth as an add or multiply alone. This is very commonly-used in linear algebra.

\subsection{Matrix Multiplication}

Matrix multiplication is probably the most well-studied algorithm, as it appears very frequently in scientific computing. In addition, matrix multiplication benefits greatly from optimization. A matrix is a 2-D array of elements, but it is stored in memory 1-D. The default in C is to store by rows, and in Fortran is to store by columns. Running down a column of a row-major matrix is very expensive, and hence the programming language used has an important impact on matrix multiplication algorithms. If the matrix size is not a multiple of the cache line size, you can get complicated behavior.

Assuming that there are only two levels of memory hierarchy, and that all the data is initially in slow memory, that \(m\) is the number of memory elements (words) moved between fast and slow memory, \(t_m\) and \(t_f\) the slow and fast memory operation times, \(f\) the number of arithmetic operations, and \(q=f/m\) the average number of flops per slow memory access, or \gls{ai}. The minimum possible run time is \(ft_f\), which occurs when all the data can be held in fast memory. But, the actual time is:

\begin{equation}
\begin{aligned}
\textrm{actual time}=& ft_f+mt_m\\
=& ft_f\left(1+\frac{t_m}{t_f}\frac{1}{q}\right)\\
\end{aligned}
\end{equation}

\(t_m/t_f\) is a purely hardware characteristic, and is referred to as the machine balance. To get to half of the peak speed, \(q\geq t_m/t_f\), where the machine balance is typically in the range of 5 to 40. This analysis ignored, however, the fact that arithmetic operations can overlap memory operations - you wouldn't add the memory and flop times, you would instead take their maximum.

\begin{enumerate}
\item Two-loop matrix-vector multiplication (DGEMV):

\begin{lstlisting}[basicstyle=\ttfamily\small]
// read x into fast memory
// read y into fast memory
for (int i = 0; i < n; ++i)
{
	// read row i of A into fast memory
	for (int j = 0; j < n; ++j)
	{
		y(i) = y(i) + A(i, j) * x(j);
	}
// write y back to slow memory
}
\end{lstlisting}

Assuming that \(x\) and \(y\) can be read into fast memory, we have three read/write operations performed on \(x\) and \(y\), and \(n^2\) read operations for the matrix \(A\) (since it has \(n^2\) total values). So, we have \(3n+n^2\) slow memory references. We have one multiply and one add for each loop, and there are \(n^2\) total loop evaluations, so we have \(2n^2\) arithmetic operations. So, \(q=(3n+n^2)/2n^2\approx2\). \(q\) must be anywhere from 5 to 40 to achieve 50\% of peak speed for most machines. This simple analysis ignored the parallelism between memory and arithmetic in the processor - some analyses drop the arithmetic term entirely, since the memory and arithmetic usually occur in parallel, and only the maximum of these run times is the actual run time (and because the memory operations are so slow, it will almost always be memory-dominated). Matrix-vector multiplication tends to run right at the peak bandwidth of the machine, since the operation is so memory-dominated (not a lot of reuse of information).

\item Three-loop matrix-matrix multiplication (DGEMM): 

\begin{lstlisting}[basicstyle=\ttfamily\small]
for (int i = 0; i < n; ++i)
{
	// read row i of A into fast memory
	for (int j = 0; j < n; ++j)
	{	
		// read C(i, j) into fast memory
		// read column j of B into fast memory
		for (int k = 0; k < n; ++k)
		{
			C(i, j) = C(i, j) + A(i, k) * B(k, j);
		}
	}
// write C(i, j) back to slow memory
}
\end{lstlisting}

Because there is one multiplication and one add, which occurs in the \(n^3\) total loops, this algorithm has \(2n^3\) flops. Each column of \(B\) is read in to \(n^2\) loops, giving \(n^3\) memory operations, each row of \(A\) is read in once, giving \(n^2\) operations, and each element of \(C\) is read/written one time, giving \(2n^2\) operations. The total number of memory operations is therefore \(n^3+3n^2\). \(q=2n^3/(n^3+3n^2)\approx 2\), which does not give any advantage over matrix-vector multiply.

Matrix-matrix multiply has much greater algorithmic intensity than matrix-vector multiply because values are reused during the multiplication process, while values can only be used once during matrix-vector multiplication (matrix-vector multiplication necessarily has a lower \gls{ai}). Matrix-matrix multiplication is more limited by the peak performance than the bandwidth of the machine.

\item Blocked (tiled) matrix-multiply (DGEMM):

\begin{lstlisting}[basicstyle=\ttfamily\small]
for (int i = 0; i < n/N; ++i)
{
	for (int j = 0; j < n/N; ++j)
	{	
		// read block C(i, j) into fast memory
		for (int k = 0; k < n/N; ++k)
		{
			// read block A(i, k) into fast memory
			// read block B(k, j) into fast memory
			
			// itself contains 3 nested loops containing the matrix-multiplication
			C(i, j) = C(i, j) + A(i, k) * B(k, j); 
		}
	}
// write block C(i, j) back to slow memory
}
\end{lstlisting}

This is very similar to the previous algorithm, except that we only read in blocks of \(A, B, C\) into fast memory, performing matrix multiplies on the blocks. People have developed this algorithm both for cache tiling and register tiling (though register tiling will look somewhat different from cache tiling because you may just want to write the multiplication of the blocks by hand because they are so small - this is referred to as ``loop unrolling''). This takes advantage of temporal locality by repeatedly using the values of the matrices until they have all been used up, and only then reading in more information. 

Each block of \(C\) is read/written \(n^2\) times, giving \(2n^2\) memory operations. Each block of \(A\) and \(B\) are of size \(b^2\), where \(b\) is the block size. This will be read in \(N^3\) times, so that for each \(A\) and \(B\), the slow memory operations are \(Nn^2\). The total number of memory operations is therefore \((2N+2)n^2\), so the \gls{ai} is \(q=2n^3/((2N+2)n^2)\approx n/N\). So, we can improve the performance by increasing the block size. But, you cannot make the block sizes arbitrarily large, since all three matrices must fit in fast memory. If we have \(M_{fast}\) fast memory, then \(3b^2\leq M_{fast}\) for this tiling algorithm to be efficient. Rearranging, \(q\approx b\leq (M_{fast}/3)^{1/2}\) shows that the fast memory size limits the possible algorithmic intensity. To run at half peak speed, \(q=t_m/t_f\), so plugging this into the previous expression will give the approximate fast memory required. This required size is reasonable for the L1-cache, but not for the registers. The lower bound for matrix-matrix multiplication is given by \(q=(M_{fast}/3)^{1/2}\), so the computational intensity is bounded by the fast memory size. This lower bound also extends to anything similar enough to the three nested loop structure of matrix-matrix multiply.

Each of the matrix-multiplies contains three nested loops. There are three nested loops for each level of memory (including slow memory), where the block for each level of memory is assumed to fit (?). The matrix block sizes are machine-dependent. The blocks cannot be too large, or else they won't fit in the cache, but if they are too small, you algorithm loses \gls{ai}. For strange block sizes, if you have a direct-mapped cache, you may have more cache misses due to interference. Often, you select non-square block sizes to take advantage of row-major or column-major matrices, since reading in one direction will be must faster than in the other. The block sizes may not have symmetric behavior (2x3 behaves differently from 3x2) due to differences in row and column major. Too small of blocks is usually not an issue.

Note that the reason why we are seeking to run at half of peak speed is that we are not accounting for the fact that single processors have some parallelism in that they can perform arithmetic and memory operations at the same time. Assuming our program is split 50/50 between these two tasks, then we should shoot for 50\% peak speed. The lower the bandwidth of your machine, the higher the amount of fast memory that you need; vice vera, higher bandwidths allow you to have smaller fast memories. The blocked algorithm will give slightly different results from the three-nested-loop algorithm due to roundoff, but this is okay. But, for this reason, most compilers will not do this optimization for you due to changes in floating point arithmetic unless you pass in a high-enough optimization flag.

\item Because we need to minimize communication between \textit{all} levels of memory, the tiled algorithm may not be a good choice because it requires the selection of a good block size. Cache Oblivious Algorithms treat a matrix multiplication as a set of smaller problems that, if divided recursively, will eventually fit in the cache. These algorithms will asymptotically minimize the amount of data transferred between every level of the memory hierarchy, and are oblivious to the number and size of the levels.

\begin{lstlisting}[basicstyle=\ttfamily\small]
double func RMM(A, B, n)
{
	C11 = RMM(A11, B11, n/2) + RMM(A12, B21, n/2);
	C12 = RMM(A11, B12, n/2) + RMM(A12, B22, n/2);
	C21 = RMM(A21, B11, n/2) + RMM(A22, B21, n/2);
	C22 = RMM(A21, B12, n/2) + RMM(A22, B22, n/2);
}

C = RMM(A, B, n);
\end{lstlisting}

The required flops is 8 times the number of operations required for multiplication of matrices of size \(n/2\) plus \(4(n/2)^2\), the required number of additions of matrices of size \(n/2\). By a geometric series, the number of operations is \(2n^3\), which is the same as with the three nested loops. The number of slow memory operations is 8 times the data movement cost of each of the subproblems of size \(n/2\) plus \(4\cdot3(n/2)^2\) due to the four adds of the three matrices \(A, B, C\) that we assume we have to load into fast memory from slow memory for each loop (assuming none of the matrices can fit into fast memory - otherwise, this contribution would be \(3n^2\)). Once the three matrices fit into fast memory, you no longer have any memory operation costs. This algorithm does not require knowledge of the fast memory size.

In practice, you need to cut off the recursion at some point, because for small enough matrices, the function overhead becomes significant. You also need to very carefully implement the code in the micro-kernel, which is executed once you stop recursion. Prefetching is also necessary to compete with other codes. You also won't run at the fastest speed possible because the recursion cutoff point is likely smaller than the maximum possible cutoff point where you would perfectly utilize the fast memory size.

\item Z-Morton Ordering may be used to achieve a better matrix ordering in memory. Matrices are stored as a contiguous arrangement of entries, but in the tiled method we want to access blocks at a time, which will, depending on the cache line size, require us to jump around in memory (non-zero stride). Blocked orderings use different orderings for the matrix entries - each block is in contiguous memory locations. This improves spatial locality. These orderings are sometimes implemented by copying the matrix to a new, recursive layout, but leaving the normal layout in memory in another location so that indexing the values for other purposes (such as finding \(A(4,7)\)) is easy - this is referred to as a copy optimization. Copy optimization may not always be a time-saving algorithm.

\begin{figure}[H]
\centering
\includegraphics[width=0.25\linewidth]{figures/Z-morton-ordering.pdf}
\caption{Z-Morton Ordering of a matrix in memory.}
\end{figure}

\item Strassen's method is a matrix multiplication method that is \(O(n^{log_2(7)})\), as opposed to \(O(n^3)\) flops. Where matrix multiplication normally takes 8 multiplies and 4 additions, Strassen's method uses 7 multiplies and 18 adds, which extends to larger matrices by divide and conquer. Strassen's method is actually forbidden to be used on the benchmarks for the Top 500 computers, since the benchmarks are designed to test the computer speed, not algorithm choice. The point at which Strassen's method outperforms conventional matrix-matrix multiply is machine-dependent.

\end{enumerate}

It is possible to asymptotically achieve lower than \(O(n^3)\) operations, but aside from Strassen's method, these asymptotic methods requires excessively high matrix sizes to overcome the leading coefficients in the order-of-magnitude estimates. However, matrix multiplication can't get any better than \(O(n^2)\), since you must at least read in the matrices.

\subsection{Other Performance Tips}

\begin{itemize}
\item Remove false dependencies in the code. For two variables, the compiler will not know if they are pointing to the same location in memory. If two variables actually point to the same location, you don't want to read in both values. With Fortran, you can pass two variables into a function, which tells the compiler that neither of the variables is an alias to the other. In C, the restrict keyword can be used on pointers.
\item Try to get variables that you're going to use very often into registers, instead of the cache - the compiler may not do this automatically. For example, ahead of a loop that is going to repeatedly use some variables, assign those variables other names just outside the loop so that they are already stored in the registers before entering the loop. There is also a {\tt register} keyword in some languages.
\item Use loop unrolling, where you explicitly type out instructions instead of having extra language features such as loops that have their own overhead. This can be tricky, however, and you might need cleanup code to account for a loss of logical operations. You can also fill the instruction cache (holds the assembly language instructions) if you have too many instructions in your code. As opposed to the instruction cache, the data cache contains the L1, L2, etc. caches.
\item Reduce instruction latency by writing operations in the correct order to take advantage of \gls{ilp}.
\end{itemize}

\subsection{Basic Linear Algebra Subroutines}

The \gls{blas} is an industry standard for how to perform certain linear algebra actions in your code, which began in the 1970s because the compiler didn't always do a good job of optimizing code. In addition, \gls{blas} was used to ensure correctness, and to ensure that you don't exceed overflow or underflow by testing for these rare events. Further versions of \gls{blas} performed the operations with higher computational intensity to achieve faster run times. Then, all linear algebra libraries, such as LAPACK, used \gls{blas} to write their subroutines.

\section{Parallel Programming Models}

While there is not a one-to-one correspondence between the machine type and the programming model, there are general trends that work best. Historically, when people developed a new parallel machine, they also developed a new programming model. This contributed to the delay in accepting parallel programming due to the complications. During the development of programming models, it was important that the software be portable among different machines, but recently that has been a redux in that we are now developing machine-specific models because people are starting to use \gls{gpu}s. However, there are many standards that have been enacted to make sure \gls{mpi} can run correctly 10 years in the future.  

A parallel machine has multiple processors, multiple memory locations, and an interconnection network that connects everything together. The interconnection network, if simple, is sometimes referred to as a bus. A torus is a nearest-neighbor mesh (Hopper), where processors only communicate with those nearby. A dragonfly topology (Edison)...

\subsection{Shared Memory}

Shared memory is the most common parallel machine hardware, and also the easiest to program - most laptops are shared memory systems. A laptop contains multiple cores, with one shared memory. This parallel machine hardware is the most difficult to scale to large processors, and scaling becomes dramatically more difficult beyond 32 processors. A program is a collection of threads of control - each thread is essentially a processor. You can dynamically create more threads at runtime, and act as if you have more processors than there actually are. You begin by calling an initially-sequential program, and then initiate new threads. Each thread has a set of private variables (local stack variables). All threads access a shared memory, which includes static variables (the shared variables), shared common blocks, or the global heap. Threads communicate implicitly by writing and reading shared variables, and coordinate by synchronizing on shared variables. When a processor accesses something in shared memory, it makes a copy of that data into its own local memory before doing any actions with that data. Then, any writes to be made to the shared variable are done to the shared location. All computations take place in private registers.

A race condition, or data race, occurs when two threads (or two processors) access the same variable and at least one thread does a write. Because the action is performed on the register, which sits right next to the processor, two threads should not attempt to write to the same value at the same time because it is possible that the two writes would happen simultaneously. These accesses, if not controlled, are not synchronized, and at least one write will be lost, even though the hardware makes sure nothing catastrophic occurs if two threads try to write to the same location at the same time. The atomic operations are reads and writes, and \textit{not} algebraic operations such as additions. To avoid race conditions, put locks in the code ({\tt static lock lk;}) - only one thread can hold a lock at a time, which prevents simultaneous read/write operations. Most work should not be in the critical region (the locked region). Some libraries include locks hidden in their functions so that you don't have to worry about specifying them.

\gls{openmp} is used to program shared memory at the CPU level. Each \gls{openmp} thread corresponds to the same program being run on another processor on the same computer chip.

Some caches are located right next to the processor. There is often a writeback cache, which stores a variable until the program is completely done with it, in which case then it is sent over the bus interconnect and is written to main memory. Sometimes, a ``write-through'' is performed, where a value is simultaneously written to the writeback cache and the main memory so that other processors can see it. This leads to the possibility of there being two copies of the same variable, however. A cache coherence protocol allows the processors to ``listen'' to the bus and see if other processors have made changes that would affect the values of that data that might be held by other processors as well. This is not scalable beyond a maximum of 100 processors, however, due to capacity and bandwidth limitations. Shared memory systems implement some type of cache coherency policy. Cache coherence logic is the reason why shared memory systems cannot scale to large numbers of processors. Sequential consistency intuition is the idea that your program should behave the same if only one processor were performing an action at any one time (though you could interleave processor actions in any arbitrary order). 

Frequent writes to a variable can cause a bottleneck. 

\subsubsection{Shared Memory}

The most common variant of the shared memory programming model is the multi-core chip model that dominates laptops. All the processors in a shared memory machine are connected to a large shared memory - these processors are typically called \gls{smp}s because there are multiple processors to a chip and they are ``symmetric'' in that each processor has somewhat of an equal role. Each chip has multiple cores, but all the caches are shared (but each processor also has its own cache). The bus is the interconnect, which acts like the lock that synchronizes the processors. 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/shared-memory.pdf}
\caption{A shared memory machine.}
\end{figure}

Example vendors of this memory system include SGI, Sun, HP, Intel, AMD, and IBM. This machine has \gls{uma}, which means that the data shared by the processors does not take longer to access if not in the local cache for one processor than for another. The shared bus is the largest bottleneck, which is why this cannot scale to many processors. The hardware keeps the caches coherent, meaning that if one processor has outdated data, then cache protocol rules implement extra coherency traffic to make sure that any processor that needs data has the most up-to-date data. This coherency is not only a waiting bottleneck, but requires extra operations. Having a larger number of processors will not necessarily lead to a proportional improvement in overall runtime, since those processors, for large enough problems, will have to all access shared memory.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{figures/shared-memory-uma.pdf}
\caption{A shared memory machine with either \gls{uma} or \gls{numa}.}
\end{figure}

Virtually no computers in the Top 500 are \textit{only} shared memory machines. Most computers on the Top 500 are clusters of nodes, where each node is programmed as a shared memory machine. Most of the best computers also have accelerators such as the Intel Xeon Phi and the Nvidia Kepler. An ``accelerator'' is a general term for something attached on the side of a CPU with communication with the CPU through a \gls{pci} - a GPU is a particular type of accelerator.

\subsubsection{Multithreaded Processor}

Each processor is not responsible for running a single subroutine, but rather, it can run many subroutines. A single processor may be assigned multiple threads to execute - when one thread reaches a load, that processor will switch over and begin executing one of its other threads, while the load occurs in the background in parallel. For multithreaded processors, there are a given number of threads that can be assigned to each core, so that the total number of things that could be happening at once is the number of threads per core multiplied by the number of cores. One function execution may be spread over multiple cores. This model is also difficult to scale up to many processors.

\subsubsection{Distributed Shared Memory}

Memory is logically shared, but physically separated. Any processor can access any address in memory. When accessing something not on its private cache, a processor will look in a table to determine in which memory location that data is stored. The information might be in its local memory or further away - cache lines, or pages, are passed around the machine. The cache line size or page size must be sufficiently large to amortize the overhead of the lookup table. This model has the same cache coherency problems as the shared memory model, and hence only scales up to 512 processors. 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/distributed-shared-memory.pdf}
\caption{A distributed shared memory machine.}
\end{figure}

\subsection{Message Passing}

All the computers on the Top 500 list are message passing machines. Every processor has its own memory, and processors communicate by sending messages to each other. This hardware format can scale to arbitrarily many processors. This is also one of the most portable ways of programming, since there are standards for sending and receiving data.

A program consists of a collection of named processes that are usually fixed at program startup time. Basically, one program is executed by all the processors. There is no shared data at all - each processor has its local data that is used for its own operations. Processes communicate by explicit send/receive pairs over a network. \gls{mpi} is the standard way for executing message passing. However, the continued use of \gls{mpi} may discourage innovation. Hybrid computers will program shared memory at the local level and \gls{mpi} at higher levels to allow faster operation.

A deadlock occurs when one processor attempts to send to another, but that processor never writes a receive statement, so the sending processor waits until its message is received. Avoiding deadlock becomes more difficult when you have more than two processors. 

\subsubsection{Distributed Memory}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/distributed-memory.pdf}
\caption{A distributed memory machine.}
\end{figure}

A distributed memory model has processors each with their own memory, and this machine model is the most common form of message passing. Then, the processors communicate through an interconnection network. Most of the Top 500 computers are distributed memory machines, but the nodes as \gls{smp}s. Each processor has its own memory and cache, but cannot directly access another processor's memory. The \gls{ni} chip allows a processor to access the memory of another processor, and controls the message passing for a processor. This separation of roles allows distributed memory to scale to arbitrarily large numbers of processors.

This machine model began development in 1994 by the simple connection of multiple computers to each other using ethernets. This PC cluster was called Beowulf, and was very cost effective since it simple combined off-the-shelf parts together. 

\subsubsection{Internet and Grid Computing}

The grid is a collection of computers connected over the internet. This is the largest parallel machine in the world. BOINC has 3.3 million hosts, with about 1000 CPU years per day. 

\subsection{Global Address Space}

This attempts to give the illusion of shared memory, even though this runs on a distributed memory machine. This is an intermediate point between shared memory and message passing. This allows scaling to arbitrarily many processors. A program consists of a collection of named threads, where each thread runs on a separate processor. There is both local and shared data, but shared data is partitioned over local processes. All the complexity of sends and receives are hidden from the programmer. This can make it difficult to think about locality if it is less obvious where the data is stored. Examples include \gls{upc}, Titanium, and Co-Array Fortran. 

\subsubsection{Global Address Space}

The global address space machine model can be run on any distributed memory machine that has \gls{ni} cards. The \gls{ni} do the communication between the processors - instead of loads/stores, you do puts/gets using one-sided communication. Processors are not interrupted by data transfer by the \gls{ni}s. There is still the potential for race conditions. 

\subsection{Data Parallel}

A single thread of control consists of parallel operations. This is easy to program because coordination is implicit - statements are executed synchronously. However, not all problems fit this model, and this is difficult to map onto coarse-grained machines.

\subsubsection{SIMD}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{figures/simd.pdf}
\caption{A \gls{simd} machine.}
\end{figure}

\gls{simd} systems consists of a large number of usually small processors. A single control processor issues each instruction, and each processor then executes the same instruction. Originally, these machines were specialized to scientific computing. This programming model can be implemented directly in the compiler, but can become difficult when there is more parallelism than processors available. 

\gls{simd} instructions use a single instruction to perform the same operation on multiple pieces of data, while scalar operations issue one instruction per operation. Unfortunately, \gls{simd} instructions can only be used for predefined processing patterns - \gls{simd} instructions cannot be used to process multiple data in different ways (such as an add alongside a subtraction). 

Data types used for \gls{simd} operations are called vector types. The length of a vector type is the length that fits into a register - 128 bits on \gls{sse} and 256 bits on \gls{avx}. So, double precision \gls{simd} operations provide the smallest degree of possible benefit from \gls{simd} instructions, since only four double precision values can be stored in a register at one time. To convert data to vector type, you need to cast a pointer to the scalar to a pointer to the vector type. 

Taking advantage of vectorization is essential to obtain fast programs. Compilers will automatically try to vectorize your loops, but the compiler may not be able to do so if you have loop dependencies, where you need previous values for future iterations. Conditional statements within loops will also prevent compiler vectorization. For Intel compilers, you can pass the flag {\tt -qopt-report=3 -qopt-report-phase=vec}, and for GCC compilers, {\tt -ftree-vectorize -fopt-info-vec-missed} to investigate which loops were not vectorized. You can also use pragmas, which are directives in C that allow you to pass additional information to the compiler.

In C/C++, the main thing that prevents vectorization is possible aliasing in pointers. Use the {\tt restrict} keyword for functions that accept multiple arrays or pointers. C++ doesn't have a {\tt restrict} keyword, but most compilers support {\tt \_\_restrict\_\_}. For instance, if you pass in two pointers to a function, where one pointer is pointing to the second element of the other pointer, then there is loop dependency, since any changes to a value of the first pointer also change the value of the second pointer. This prevents loop unrolling.

A \gls{simd} system consists of a large number of (usually) small processors with a single control processor that issues the instructions. Each processor executes the same instruction. Branches result in some processors being turned off. A useful way to think about programming data parallel machines is to assume you have an unlimited number of processors, because this should help you abstract away processor limitations. 

\subsubsubsection{SSE}

\gls{sse} had registers that were 128 bits each. 

\subsubsubsection{AVX}

Intel \gls{avx} is a set of instructions for doing \gls{simd} on Intel CPUs. The width of the \gls{simd} register, of which there are 16, was increased from 128 bits (legacy \gls{sse}) to 256 bits (32 bytes) in Intel \gls{avx}. An example of this type of architecture is the Intel Xeon Sandy-Bridge or Intel Ivy Bridge, which can perform four double precision operations concurrently. Some instructions take four-register operands. 

In addition, three-operand, nondestructive operations were added, so that an operation such as \(A=A+B\), which would overwrite \(A\) in the process, can now be written as \(C=A+B\), leaving the original operands unchanged. In addition, \gls{fma} was added. 

\subsubsubsection{Intel Xeon Phi}

The Intel Xeon Phi can perform 8 double precision operations concurrently. A 512-bit register is now typically the norm for new computer chips that are being developed.

\subsubsection{Vector}

Vector machines are based on a single processor, with multiple functional units that can perform the same arithmetic operation at the same time. Some processors can be turned on or off. Vector machines were overtaken by MPPs in the 1990s where you could connect many off-the-shelf processors, but they are re-emerging in recent years. 

\subsubsection{GPU}

GPUs are essentially a version of vector architecture. The key idea is that the compiler does some of the work of finding parallelism so that the hardware doesn't have to. 

Companies originally developed their own programming languages - Nvidia uses CUDA for programming on GPUs. While \gls{mpi} is now the standard for programming on distributed memory machines, the GPU community is now selecting a standard for programming with GPUs. OpenCL is the standard for programming with GPUs. 

\subsection{Hybrid}

The largest machines on the Top 500 list are actually clusters of the machines listed previously - they are heterogeneous. 14\% of the Top 500 list have accelerators, and those computers accounted for 35\% of the performance. Multicore/\gls{smp}s are a building block for a larger machine with a network. Each node is a multicore chip with many processors. The old name for this type of hybrid machine was \gls{clump}. The simplest way to program this type of machine is to treat the machine as ``flat,'' and use one \gls{mpi} process per core. Using \gls{mpi} to communicate between two processors on the same chip is a waste of resources, however, so shared memory programming should be done within on \gls{smp}, but message passing outside the \gls{smp}. One core will be dedicated to performing \gls{mpi} with all the other processors on the machine. 

DARPA was involved in a project to develop a programming language to hide the duality between shared memory programming at a \gls{smp} but \gls{mpi} between processors, though this has not yet taken off. Global address space models can often call message passing libraries and vice versa. 

Within a single node, run \gls{openmp}, and between nodes, run \gls{mpi}. It appears that the best performance is obtained by running two or four \gls{mpi} processes per node. \gls{openmp} does not scale well to high numbers of cores due to the fork/join actions that create bottlenecks. It is generally better to reduce the number of \gls{mpi} processes in favor of increasing more \gls{openmp} processes, but you want to obtain a balance between the two.

\subsection{Cloud Computing}

Cloud computing allows people to easily shared thousands of computers. You pay a fee to access these resources.

\section{Sources of Parallelism and Locality in Simulation}

There is often a great deal of independence between objects, and objects tend to depend much more on nearby objects than on distant objects. Dependence on distant objects can also often be simplified. In addition, when a continuous domain is discretized, time dependencies are generally limited to adjacent time steps. 

\subsection{Discrete Event Systems}

The objects, state, time, etc. are all discrete. The set of all variables at a given time is called the state. Each variable is updated by computing a transition function depending on the other variables. These systems can be synchronous or asynchronous. For synchronous systems, all transition functions are evaluated at each time step - this is also called a state machine. At each time step, the old state is read, and the new state is written, so there is no possibility for a race condition. This doubles the required memory, however. 

The asynchronous system only evaluates the transition function if the inputs have changed - this is also referred to as event-driven simulation. This is much cheaper than always looking to your neighbors to see if something has changed from the previous time steps. Every event is associated with a time stamp, but there is no global time step. Asynchronous simulation is much more efficient, but more difficult to parallelize because it's more difficult to decide when to ``receive.'' Conservative asynchronous simulations only simulate up to and including the minimum time stamp of the inputs. However, then you need deadlock detection if there are cycles in the graph. Speculative asynchronous simulations, on the other hand, assume that no new inputs will arrive, and keep simulating. You may need to backup if this assumption is wrong. 

Locality is achieved using domain decomposition, where the domain should be split up so as to minimize the amount of communication required between pieces of the domain. Basically, you want to minimize the surface to volume ratio. Graph theory is used to find the optimal domain decomposition, and graph partitioning is the process of assigning subgraphs to the different processors. You might not simultaneously be able to achieve load balancing with minimal communication between processors. Use a quad-tree (2-D division by recursive rectangle division) or an oct-tree (3-D) to initially obtain a good balance between load balancing and minimal communication. This is an NP-hard algorithm, which means that the optimal algorithm requires an exponential number of processors. Domain decomposition is more complicated for particle systems if there are far-field forces, however, since every particle still impacts every other particle - this is usually accounted for by passing around each processor's particle to all the others. 

\subsection{Particle Systems}

Particle systems are essentially a version of lumped systems. Time is now continuous, but your system is largely discrete because you have a finite number of objects. Forces on a particle can be divided into the external force, the nearby force, and the far-field force. External forces can be computed regardless of the other particles, and are embarrassingly parallel. Far-field forces can often be approximated as simpler forms; far-field forces are typically governed by elliptic PDEs. Nearby forces are the most difficult to parallelize. In addition, for particle systems, it is very easy to distribute an even number of particles amongst the processors to achieve good load balancing and locality.

Parallelizing nearby forces scales as \(n^2\) if you need to look at all pairs of particles to determine the nearby forces - domain decomposition allows this to be simplified. The ``ghost zone'' is the boundary region between all separate domains. 

\subsubsection{Particle Mesh Methods}

Superimpose a mesh on the particles, and move the particles to the nearest grid point. Far field forces are easy to solve on a regular mesh, so the complexity drops to \(O(nlog(n)\) instead of \(n^2\). This method is widely-used.

\subsubsection{Tree Decomposition}

Forces from a group of far-away particles are simplified to a single ``large'' particle. Each node on the tree contains an approximation of its descendants. This method also scales as \(nlog(n)\) instead of \(n^2\) for approximating far-field forces. 

\subsection{ODEs}

Entities are discrete, but time is continuous - ODEs represent lumped systems, because space is not continuous. 

\subsection{PDEs}

Everything is continuous - both space and time. Elliptic equations are generally steady-state, with global space dependence. Hyperbolic equations are generally time-dependent with local space dependence (finite wave speeds limit communication with distant regions of the domain). Parabolic equation are generally time-dependent with global spatial dependence. In order of increasing difficulty of parallelization, these equations ranks 1) elliptic, 2) hyperbolic, and 3) parabolic. Global dependence results in either a lot of communication or very small time steps. From a numerical stability point of view, hyperbolic equations are the most difficult to solve. 

Explicit methods only require a matrix-vector multiplication, while implicit methods require a matrix solve at each time step. Iterative solvers convert this required matrix solve into matrix-vector multiplications.

\subsection{SpMV}

\gls{spmv} is most commonly performed in a \gls{csr} format. You only want to store and do arithmetic on nonzero entries. All the data is stored in one array, where only the nonzero values are stored. Then, you have two additional arrays - one holding pointers to the rows, and one holding pointers to the columns. The column vector holds pointers that indicate which positions in each row (i.e. the column numbers) are nonzero. Each entry in the row vector indicates the position in the data vector that is the first nonzero value in that row. This can give a lot of cache misses in the resultant vector if the nonzero values are not clustered along the diagonal (the ideal structure for parallelization is a block diagonal matrix). There are algorithms to cluster the nonzeros together to avoid this cache miss problem. A potential issue with parallelization is that all the processors might have to access every entry in the vector \(x\). Instead of breaking up the rows into blocks, you could split them up however you want for the processors to minimize the accesses to \(x\). Locality also improves when there are few nonzeros off the diagonal. The reason that you have communication with respect to accessing \(x\), you generally want to avoid copying data to each processor because this will eat up your memory.

Applying \gls{ge} or cholesky decomposition will fill in a matrix with zeros. A matrix can be reordered to reduce this fill-in if you intend to use \gls{ge}.

\subsection{Graph Partitioning}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{figures/graph-partitioning.pdf}
\caption{An illustration of the relationship between graph partitioning and a matrix.}
\end{figure}

A sparse matrix and a graph are different representations of the same concept - edges in the graph are nonzero in the matrix. The edges of the graph become off-diagonal terms.

\section{POSIX}

\gls{posix} is an interface to operating system utilities. PThreads is the \gls{posix} threading interface. These are system-level calls that are use to create parallelism and synchronize threads. They can be used for multiple languages. However, the overhead of thread creation is large (much larger than for \gls{openmp}), and data race conditions are very difficult to debug. \gls{openmp} is the alternative to \gls{posix} PThreads.
 
\section{OpenMP}

Multiprocessor machines initially improved performance through clock rate increases, but this has slowed down, so now performance is enhanced by increasing the number of processors per chip.

In the early 1990s, vendors of shared memory machines provided their own, directive-based Fortran programming extensions that the users would use in their code to be used by the compiler to determine where to parallelize. The \gls{openmp} standard specification began in 1997 to allow the multitude of relatively similar \gls{smp} parallelization languages to be condensed into a single language. \gls{openmp} continues to be developed - in 2015, version 4.5 was released.

\gls{openmp} in an \gls{api} for programming shared memory processing in C, C++, and Fortran. \gls{openmp} is used for parallelism within a (multi-core) node, while \gls{mpi} is used for parallelism between nodes. \gls{openmp} is therefore intended for shared memory programming. \gls{openmp} is an implementation of multithreading, where a master thread (a series of instructions executed consecutively) forks a specified number of slave threads and the system divides a task among them. The threads then run simultaneously. The runtime environment allocates threads to the different processors. Each thread executes the parallelize section of code independently of all other threads. \gls{openmp} functions are included in the {\tt omp.h} header file in C/C++ - compiling with the {\tt -fopenmp} option will allow you to use the pragmas. \gls{openmp} consists of compiler directives (pragmas), runtime library routines, and environment variables. In C/C++, all of the library routine are truly routines (they don't return a value - \textit{functions} are usually defined as returning something). 

\gls{openmp} is a feature of the compiler, and hence it is not downloaded as other software might be. \gls{openmp} uses pragmas to perform actions - the most important pragmas are described below. To compile with gcc, you need to pass in the {\tt -fopenmp} option to {\tt g++}. With \gls{openmp} compilation, the {\tt \_OPENMP} macro becomes defined.

\gls{openmp} is not required to check for data dependencies, data conflicts, race conditions, deadlocks, or code sequences that would make serial and parallel execution results differ. The programmer is also responsible for synchronizing input and output. \gls{openmp} is useful from the point of view of incrementally parallelizing an application, which differs from message-passing libraries which usually require an all-or-nothing approach. 

\gls{openmp} can program both \gls{uma} and \gls{numa} shared memory machines. 

\gls{openmp} programs achieve parallelism exclusively through the use of threads. A thread of execution is the smallest unit of processing that can be schedule by the operating system. Usually, the number of threads matches the number of processors. \gls{openmp} is an explicit (not automatic) programming model, which offers the programmer complete control over the degree of parallelism. \gls{openmp} uses the fork-join model of parallel execution - the master thread will fork into the number of requested threads for a parallel region - after the parallel region has ended, the threads collapse back into a single master thread, which may be again divided into multiple threads in later parallel regions. The parallel threads in a parallel region are referred to as a \textit{team}. \gls{openmp} supports nested parallelism, where parallel regions can be created inside of other parallel regions. The \gls{api} also allows the runtime environment to dynamically allocate threads for the parallel regions, which can help to more efficiently use resources. \gls{openmp} says nothing about parallel I/O, however, which is important if multiple threads try to read or write from the same location in memory.

Threads are not required to maintain exact consistency with the shared memory - the programmer must ensure that threads that are trying to access a shared variable, which might have changed on some other processor since the last access, are attempting to access a variable only after it has been flushed by all threads.

Local thread variables are stored on the stack, while shared variables are generally on the heap.

\subsection{Compiler Directives}

All compiler directives for C/C++ begin with {\tt \#pragma omp}, following by the directive name, then any clauses that there may be (you must have a directive, but don't necessarily need clauses). A newline must follow every directive. Only one directive name can be specified per directive. Each directive applies to at most one succeeding statement, which must be a structured block. 

\subsubsection{Thread Creation}

The pragma {\tt \#pragma omp parallel} is used to fork threads to carry out the work in the construct in parallel. The original thread, also known as the master thread, has an ID of 0. The following code will execute the {\tt printf} statement on every available processor, which is four for my computer. The output may not exactly print ``Hello world.'' four times consecutively due to the possibility of race conditions. The (integer) thread number can be determined using {\tt omp\_get\_thread\_num()}.

\begin{lstlisting}[language=C, basicstyle=\ttfamily\small]
#include <stdio.h> // For: printf

int main(void) {
	#pragma omp parallel
	printf("Hello world.");
}
\end{lstlisting}

When a thread reaches a {\tt \#pragma omp parallel} directive, it creates a team of threads and becomes the master of the team. The master is itself a member of that team and has thread number 0 within that team. Beginning from the start of this parallel region, the code is duplicated and all threads execute that code. All parallel regions have an implied barrier at the end - only the master thread continues execution past the barrier. If any thread terminates within a parallel region (fails for some reason0, then all threads in the team will terminate, giving undefined behavior. Threads are numbered beginning at 0. At the entry and exit of a parallel region, an implicit flush is performed.

A parallel region must be a structured block that does not span multiple routines or source files. You cannot branch into or out of a parallel region. 

\subsubsection{Work-sharing}

Work-sharing constructs are used to specify how work is to be divided amongst the processors - a work-sharing construct divides the execution of the enclosed code region among the threads that encounter it. Work-sharing does not launch new threads - they only divide up the work. There is no implied barrier at the entry to a work-sharing construct, but there is a barrier at the end. Work-sharing constructs must be enclosed within a parallel region for them to be executed in parallel - otherwise, only the master thread executes them. Work-sharing constructs must be encountered by all members of a team or none at all, and successive work-sharing constructs must be encountered in the same order by all members. 

If all the iterations of a loop can be performed independently of one another, then {\tt omp for} or {\tt omp do} automatically split up the loop to the number of available processors. The following code will print the numbers 0 to 11 in a random order, since the printing is split up amongst the processors. The variable {\tt i} is copied to each processor, and is local to each processor.

\begin{lstlisting}[language=C, basicstyle=\ttfamily\small]
int main(void) {
	#pragma omp parallel for
	for (int i = 0; i < 12; ++i)
		printf("%i", i);
}
\end{lstlisting}

\begin{itemize}
\item {\tt for, do}: split up (independent) for-loop iterations among the threads. This represents a form of data parallelism, and assumes that a parallel region has already been defined, or else this executes in serial. The {\tt do} loop cannot be a do-while loop or a loop without loop control. An implicit flush is performed upon exit. \gls{spmd} is a programming technique that splits up a task amongst multiple processors and has them each perform a section of the task by passing the sections in as arguments to the task - the for construct is very similar to this. If already within a parallel section, and you don't want all of the threads in the team to execute the for loop, then do not include {\tt parallel} in the directive.
\item {\tt sections}: assign consecutive, but independent, code blocks to different threads - each section is executed by one thread. This can be used to implement functional parallelism. Independent {\tt section} directives are nested within a {\tt sections} directive, and each section is executed once by a thread in the team. It is possible for a thread to execute more than one section if it is fast enough. There is an implied barrier at the end of the {\tt sections} directive, which results in an implicit flush upon exit.
\item {\tt single}: specify a code block that is executed by only one thread - a barrier is implied at the end, meaning that all threads will pause until the block has been executed and that an implicit flush is performed. This is useful for sections of code that are not thread safe, such as data I/O. 
\item {\tt master}: the code block is only executed by the master thread, with no barrier implied at the end. Hence, all other threads skip this section of the code.
\item {\tt collapse}: how many loops in a nested loop can be collapsed into one large iteration space and divided according to the {\tt schedule} clause. This should not be used if the loops depend on each other. In a ``square loop,'' where the loop iterations are independent, without using the collapse clause, the outer loop will be divided among four processors, and each processor will perform 100 of the inner loop iterations. However, if you have more than four processors, then the other processors are wasted - using {\tt collapse(2)} would divide the 400 iterations evenly among however many processors you have.

\begin{lstlisting}[language=C, basicstyle=\ttfamily\small]
for (int i = 0; i < 4; ++i) { 
	for (int j = 0; j < 100; j++) {
		printf("Distribute this work!");
	}
}
\end{lstlisting}
\item {\tt task}: defines a task which may be executed by the thread encountering it, or be deferred for later execution by another thread in the team
\end{itemize}

Work-sharing constructs have implicit barrier synchronization at the end. For convenience, \gls{openmp} allows two combined directives, {\tt parallel do/for} and {\tt parallel sections}. If there is not enough work to justify starting up a parallel work-sharing region, \gls{openmp} may not do so.

\subsubsection{Clauses}

Because \gls{openmp} is a shared memory programming model, most variables in the code are visible to all threads by default. But sometimes, private variables can be used to avoid race conditions, and you may want to be able to specify how the variables on a processor can be seen by the other processors. Clauses can be appended to the \gls{openmp} directive to specify special behavior. By default, all variables in the work-sharing region are shared except loop iteration counters.

\subsubsubsection{Data Sharing/Data Scope Attributes}

Global variables include file scope variables and static variables. Private variables include loop index variables and stack variables in subroutines called from parallel regions.

\begin{itemize}
\item {\tt shared}: the data within a parallel region is shared, which means that the data is visible and accessible by all threads simultaneously - this is the default data attribute clause, and only loop iteration counters are not shared. A shared variable exists in only one memory location, and all threads can read/write to that address. Race conditions can result if more than one processor try to read/write at the same time, since the processors are actually pointing to the same location in memory.
\item {\tt private}: the data within the parallel region is private to each thread, meaning that each thread has a local copy of all variables, which are used as temporary variables and are typically stored on the stack.  You could alternatively pass in a list of variables to specifically be considered private. When a variable is declared private, a new object of the same type is declared once for each thread in the team. All references to the original object are replaced with references to the new object. Variables declared private should be assumed to be uninitialized for each thread. As contrasted with {\tt threadprivate} variables, private variables are defined at the start of a parallel region or a work-sharing construct. A private variable is not maintained for use outside of the parallel region. Loop iteration counters are by default private variables, and don't need to be specified with this clause.
\item {\tt threadprivate}: the data is global data, but is private in each parallel region during runtime. This type of variable differs from private variables because it is able to persist between different parallel regions of a code (retains its value upon exiting the parallel region). 
\item {\tt default}: allows the programmer to specify whether the default data scoping is to be used as opposed to {\tt none}, which would then require the programmer to manually scope every variable in the parallel region. Specific variables can be exempted. 
\item {\tt firstprivate}: the data is private to each thread, but initialized using the value of the variable of the same name from the master thread. This combines the behavior of the private clause with automatic initialization of the variables in the list. 
\item {\tt lastprivate}: the data is private to each thread. If the current iteration is the last iteration in the parallelized loop, then the value is copied to a global variable of the same name outside the parallel region.
\item {\tt copyin}: assigns the same value to {\tt threadprivate} variables for all threads in the team. The master thread is used as the copy source. 
\item {\tt copyprivate}: broadcast values acquired by a single thread directly to all instances of the private variables in the other threads. This is associated with the {\tt single} directive. 
\item {\tt reduction}: perform a reduction on the variables that appear in its list. A private copy for each list variable is created for each thread. At the end of the reduction, the reduction variable is applied to all private copies of the shared variable, and the final result is written to the global shared variable. For instance, {\tt reduction(+:result)} will perform a sum of each thread's variable {\tt result}. Reductions can only be performed for scalar variables, and not for arrays or data structures. The reduction is used only for operations such as {\tt x = x <op> <expr>, x = <expr> <op> x} (no subtraction), or any of the combined operations such as {\tt x += 4}. This defines {\tt result} as a special reduction variable - it is neither private nor shared. Local copies are reduced into a single value and combined with the original global value.
\end{itemize}

\subsubsubsection{Synchronization}

Synchronization is used to update the shared memory accessed by all processors. 

\begin{itemize}
\item {\tt critical}: the enclosed code block is only executed by one thread at a time, which eliminates race conditions with shared data. You can optionally define a name following this clause so that multiple different critical regions can exist. The names act as global identifiers, and different critical regions with the same name are treated as the same region. All critical sections that are unnamed are treated as the same section. An implicit flush is performed upon entry and exit.
\item {\tt ordered}: a structured block is executed in the order in which iterations would be executed had the block been entirely sequential. Threads will need to wait before executing their chunk of the iterations if previous iterations have not yet been completed. An {\tt ordered} directive can only appear within a {\tt for} or {\tt do} parallel region. Only one thread is allowed in an ordered section at any one time. An implicit flush is performed upon entry and exit.
\item {\tt barrier}: each thread waits until all threads have reached this point. A barrier contains implicit flushes on all threads, so any changes to variables within the parallel region will be ``noticed'' by the other threads upon the barrier. All threads will then resume executing in parallel the code that follows the barrier.
\item {\tt nowait}: threads completing assigned work can proceed without all other threads in a team waiting to finish. If there are multiple independent loops within a parallel region, then you can use this clause to avoid the implied barrier at the end of loop constructs.
\item {\tt atomic}: specifies that a memory location must be updated atomically, rather than letting multiple threads attempt to write to it. This basically creates a mini critical region, since it only applies to a single statement that immediately follows the clause.
\item {\tt flush}: identifies a synchronization point where the implementation must provide a consistent view of memory. Thread-visible variables are written back to memory. This basically says that a variable cannot be kept in the local register as opposed to in main memory. This is required even for cache coherent systems, which are systems that will automatically update all processors when one processor performs a read/write on data so that all processors are always up-to-date on the shared data. You can optionally provide a list of variables to flush so that you don't flush all variables. For pointers in this list, the pointer itself is flushed, and not the object it points to. A shared variable must be flushed by both the thread attempting to read the variable and all threads writing it before a thread can read a shared variable. You could circumvent this by declaring every shared variable as {\tt volatile}, which performs a flush before every read and write, but this disables many compiler optimizations.
\end{itemize}

\subsubsubsection{Scheduling}

\begin{itemize}
\item {\tt schedule(<type>, <chunk>)}: this assigns iterations in the work sharing construct to the threads. The default schedule is implementation-dependent. The {\tt <type>} can be:
	\begin{itemize}
		\item {\tt static}: all threads are allocated iterations before they execute the loop iterations. The iterations are by default divided evenly amongst the threads. However, specifying {\tt <chunk>} will allocate that many iterations (contiguously) for each thread.
		\item {\tt dynamic}: only some of the iterations are initially allocated, so that when a thread finishes its own allocation, it returns to get additional allocations from the iterations that are left. This can help alleviate cases when some iterations require substantially more work than others. {\tt <chunk>} now specifies the number of iterations that are allocated at a time. The default chunk size in this case is 1.
		\item {\tt guided}: this allocation is very similar to the dynamic allocation, except that a large chunk of contiguous iterations are allocated to each thread initially, and the chunk size decreases exponentially with each successive allocation to the minimum size specified by {\tt <chunk>}. The block size is proportional to the number of iterations divided by the number of threads, and then later to the number of remaining iterations to the number of threads. 
		\item {\tt runtime}: the scheduling decision is deferred until runtime, and is determined by {\tt OMP\_SCHEDULE}
		\item {\tt auto}: determine by the compiler and/or runtime system
	\end{itemize}
\item {\tt num\_threads}: specify the number of threads to allocate for this parallel work (can be lower than the total number of threads). This has higher precedence than using the {\tt omp\_num\_threads} library routine. If this is not specified, then the default used is usually the number of CPUs on the node, though it could be dynamic. The number of threads cannot be redefined within a parallel section using the {\tt omp\_set\_num\_threads()} routine (but it can be defined outside the parallel region). 
\end{itemize}

\subsubsubsection{If-control}

\begin{itemize}
\item {\tt if}: threads will parallelize the task only if a condition is met - otherwise, the block will execute serially by the master thread. Only a single if-clause can be given. 
\end{itemize}

\subsection{Environment Variables}

\gls{openmp} acts as though there are \gls{icv}s that control the behavior of the program - these variables are set using either the \gls{openmp} library routines in {\tt omp.h} (higher precedence) or typical shell commands such as {\tt export OMP\_NUM\_THREADS=8}. Then, each parallel region has its own copy of these variables. Using the environment variables as opposed to the library routines is useful for other users using a code, since they wouldn't have to go modify the source code for it to work optimally on another machine.

\subsection{Library Routines}

\begin{itemize}
\item {\tt omp\_set\_max\_active\_levels}: sets the maximum number of nested active parallel regions - there is one copy of this for the entire program
\item {\tt omp\_set\_num\_threads}: set number of threads for parallel regions
\item {\tt omp\_set\_nested}: this sets the level of nesting that is allowed for a given parallel task
\item {\tt omp\_set\_dynamic}: passing in 0 will disable the dynamic adjustment of the number of threads. Passing in a nonzero value will allow \gls{openmp} to choose any number of threads up to the maximum available, even if {\tt num\_threads} is specified.
\item {\tt omp\_get\_dynamic}: determines if dynamic threads are enabled
\item {\tt omp\_get\_nested}: determines if nested parallel regions are enabled. If not enabled, then you can still program nested parallel regions, but the entry of a nested parallel region will create a new team that consists of a single processor.
\item {\tt omp\_get\_num\_threads}: returns the number of threads that are currently in the team executing the parallel region from which it is called
\item {\tt omp\_get\_thread\_num}: returns the thread number of the thread
\item {\tt omp\_get\_thread\_limit}: returns the maximum number of threads available to a program
\item {\tt omp\_get\_num\_procs}: returns number of processors that are available to the program
\item {\tt omp\_in\_parallel}: determines if the section of code is executing in parallel
\item {\tt omp\_get\_ancestor\_thread\_num}: returns the thread number of the ancestor thread
\item {\tt omp\_get\_team\_size}: returns the size of the thread team
\item {\tt omp\_get\_active\_level}: returns the number of nested, active parallel regions 
\item {\tt omp\_init\_lock}: initializes a lock associated with the lock variable
\item {\tt omp\_destroy\_lock}: disassociates the given lock variable from any locks
\item {\tt omp\_set\_lock}: acquires ownership of a lock
\item {\tt omp\_unset\_lock}: releases a lock. Locks, as opposed to critical regions, should be used when you have different types of data structures.
\end{itemize}

\subsection{Stack Size and Thread Binding}

The \gls{openmp} standard does not specify how much stack space a thread should have, so implementations will differ in the default thread stack size. Threads that exceed their stack allocation may or may not segmentation fault - an application might continue to run while its data is being corrupted. Statically linked codes may be subject to further stack restrictions. You can set the thread stack size prior to execution using the {\tt OMP\_STACKSIZE} environment variable. 

In some cases, a program will perform better if its threads are bound to processors. Binding a thread to a processor means that a thread will be scheduled  by the operating system to always run on the same processor - contrarily, threads would be executed on any processor.  Thread binding is also referred to as ``thread/processor affinity.'' Binding threads to processors can result in much improved cache utilization by reducing memory accesses that would have to be repeated if a thread were bounced around among processors. The environment variable {\tt OMP\_PROC\_BIND} can be set to true to turn processor binding ``on.'' 

\subsection{Debugging and Performance Analysis}

TotalView is a recommended parallel debugger. For more simple analysis, the {\tt ps} Unix command shows a list of the currently running processes on your computer. The job-id is the PID, and the terminal type the TTY. {\tt ps -A} shows all of the running commands. 

\section{Cloud Computing and Big Data Processing}

Hadoop and Spark are used to program in parallel in cloud computing and big data environments. The amount of data that we need to process is growing at a rate faster than twice every 18 months (it beats out Moore's law). This means that we cannot process all of this data on a single machine (a node). Datacenters each typically have about 10,000 machines, with 12-24 hard disks per node and 256 GB RAM cache. This results in about 50 TB of storage per machine. Datacenters initially used a tree topology, which is subject to some of the links in the tree being bottlenecks (over-subscribed), so there is a lot of research being performed to determine how to achieve full bisection bandwidth, or full network speed when communicating between any two arbitrary processors. Datacenters have higher storage, but generally slower interconnect and less cores per node - most supercomputers focus on obtaining high flops/sec.

Cloud computing allows computing to be thought of as a utility. From around 2006, you can rent virtual machines in the ``cloud'' and pay as you go. The owner of those machines perform upkeep, and you simply get to use the computing power. 

\subsection{Programming Models}

The problem with programming in \gls{mpi} on big data centers is that processor failures are fairly common, and if one processor fails, then all of the other processors also fail. 10,000 nodes see about 10 faults/day (1 server fails about once every 3 years). Research is being performed on \gls{ftmpi} using checkpointing-based techniques to prevent failure of all processors if one fails.

The main idea behind programming in data center environments is to restrict the programming interface so that it automatically handles failures. You don't specify which processors talk to each other, or where a program should run - you let the program determine this. If there is a failure, the program might retry the computation on another node. If the same task repeatedly fails, end the job. Because sections of the code might be re-run, the code should be deterministic. If one machine that has a file dies, then because that file has been copied to other nodes, it can be run on other nodes. 

Some machines run slower, so you might run two copies of the same task, and simply use the output of the machine that ends first. 

\subsubsection{Hadoop}

Hadoop is an open-source project started at Yahoo and Facebook that implements functions that mimics the map-reduce model. 

\subsubsubsection{MapReduce}

In the map function, you input all of the key-value pairs in your program, and intermediate key-value pairs are output. Then, the reduce function uses all of those intermediate key-value pairs and combines them together to generate some output. The focus on \gls{mr} is how to obtain high levels of data parallelism while focusing on fault tolerance of machines and dealing with slow machines.

Most applications require multiple \gls{mr} steps, which significantly increases the code complexity. \gls{mr} is also very expensive for apps that need to reuse data, since that data must be reread each time it is to be used.

\subsubsection{Spark}

Spark allows you to perform parallel transformations on collections. This results in 5 to 10 times less code than the typical \gls{mr} code, while having portability to multiple languages. Spark uses \gls{rdd}s, which treats a dataset as a list distributed among several machines. Transformations and actions can be performed on these lists. 

\section{Data Parallel Programming}

Data parallel languages use a serial thread of control - there are no data races or nondeterminism. It simply consists of operations that can act on aggregate data structures to implicitly express parallelism. 
These are also referred to as ``collection-oriented languages'' because they are built so that they operate on collections of data (similar to acting on vectors). 

Broadcasting a value to \(p\) processors takes \(log(p)\) time and \(log(p)\) span (span is the length of the longest path). The lower bound on reductions is \(log(n)\) for functions of \(n\) input variables. 

\section{Distributed Memory Machines and Programming}

\gls{mpp} is simply a different name for a cluster. A cluster consists of more commodity components, where \gls{mpp}s are more custom-built. Distributed memory machines must be programmed with \gls{mpi}, \gls{upc}, \gls{upc}++, or some other message passing language. Initially, distributed memory machines connected multiple processors, and communication was performed using bi-directional queues between nearest neighbors. Algorithms for these machines focused on reducing the number of hops between further-away processors. To have a large number of data transfers occurring at the same time, you needed a large number of distinct wires. This is different from the bus required for shared memory systems. The network bandwidth is limited by the bit rate per wire and the number of wires. Latency, on the other hand, really cannot be improved beyond the speed of light.

Routing algorithms determine how information is passed around between processors. In a 2-D torus, messages pass east-west and north-south to avoid deadlocks. The switching strategy determines how the messages are passed around - circuit switching occurs when the full path is reserved for the entire message (like a telephone). Packet switching occurs when a message is broken into separate packets, similar to the internet. Packet switching allows greater use of wires, but requires each of those packets to have their own headers which describes where each packet is going. The flow control procedure describes what to do if there is congestion - data might stall, be stored temporarily in buffers, or be rerouted to other nodes. The source node might be instructed to temporarily halt.

The diameter is the maximum (over all pairs of nodes) of the shortest path between a given pair of nodes. The latency tends to vary widely across architectures. Vendors often report hardware latencies related purely to the wire transfer, but application programmers actually care about software latencies (user program to user program - data has to go to the network card, has to be decoded, and has to travel up the software stack). Latency differs by 1-2 orders of magnitude across network designs. The software/hardware overhead at the source/destination dominate the cost. Latency has not improved significantly, unlike Moore's law. The T3E machine had the lowest latency in the late 1990s, and thins have actually gotten slightly worse since then because it is very expensive to achieve low latency, and other factors might cause you to spend money on other improvements before latency improvements.

The effective bandwidth is usually lower than the physical bandwidth due to packet overhead that describes the routing control information. There also needs to be error information that describes what to do if the data transfer fails. A lower effective bandwidth is observed if you are sending small packets of information, since then the cost of the packet overhead is not sufficiently amortized. The bisection bandwidth is the bandwidth across the smallest cut that divides the network into two equal halves. The bisection bandwidth is important for algorithms where the processors need to communicate with all others.

\subsection{Topologies}

The topology describes how the chips are connected to each other. In the past, there was a lot of research on network topology and in mapping algorithms to the topology, and there can be significant gains by mapping the algorithm to the toplogy. You want to minimize the number of hops between nodes. Modern networks, however, hide the hop cost so the topology becomes less of a factor.

\begin{itemize}
\item Crossbar: everyone is connected to everyone. This is very expensive due to the large numbers of cables.
\item Ring/1-D torus: this is primarily used in single-node machines. 
\item higher-D mesh: 
\item higher-D torus: BlueGene Q are 5-D tori
\item Hypercube: This was popular in early machines
\item Tree: bottlenecks associated with binary trees led to the development of fat trees, which avoid bottlenecks by placing more (or wider) links near the top of the tree structure
\item Butterfly: This topology is not widely used anymore, and contains many wires. Processors are connected pairwise, and then four-wise, and then eight-wise, etc. 
\item Perfect shuffle
\item Dragon fly: This is the topology used in Edison. Optical cables, which are most expensive, go between cabinets in the machine room, have the higher bandwidth. Electrical networks are cheaper and are faster when the distance is short. The Dragonfly has groups with all-to-all links, where each group has at least one link directly to each other group. The topology within each group can be any topology. A randomized routing algorithm is used. The result is that the programmer can usually ignore the topology and still get good performance, at the cost of variable performance.
\end{itemize}

\subsection{Performance Models}

With \gls{pram}, all memory access operations are considered to be complete in one period, i.e. there is no concept of memory hierarchy. The parallel algorithm design strategy is to first do a \gls{pram} algorithm, and then worry about the memory and communication time. 

A better model is the latency and bandwidth model. The time to send a message is the sum of the latency and the length of the message multiplied by the cost per word (the inverse of the bandwidth). The topology is assumed irrelevant. This is often called the \(\alpha-\beta\) model. Usually, one long message is much cheaper than many short messages. 

\section{MPI}

\gls{mpi} was the first (and currently only) standardized, vendor-independent message passing library. Originally, there were many message passing libraries, and the best competitor to \gls{mpi} was \gls{pvm}, developed at ORNL. \gls{mpi} is basically intended for \gls{spmd} programming. \gls{mpi} is simply a specifications for the developers and user of message passing libraries - by itself, it is not a library, but rather a specification of what such a library should be. Message passing involves moving data from the address space of one process to that of another process through cooperative operations on each process. \gls{mpi} can be used with C and Fortran. Originally, \gls{mpi} was intended for distributed memory architectures, which were becoming increasingly popular in the 1980s-1990s. Then, as architecture trends changed, shared memory \gls{smp}s were combined over networks, creating hybrid distributed memory/shared memory systems. Both types of underlying memory architectures are handled seamlessly. \gls{mpi} can run on distributed memory, shared memory, and hybrid architectures, though the programming model remains a distributed memory model (regardless of the underlying physical architecture). The first version of \gls{mpi} was introduced in 1994. 

The {\tt mpi.h} header file must be included for all programs that make \gls{mpi} library calls ({\tt \#include <mpi.h>}). Parallel code begins once the \gls{mpi} environment is initialized using {\tt MPI\_Init}, and terminates once the environment is terminated using {\tt MPI\_Finalize}. \gls{mpi} calls return values - {\tt MPI\_SUCCESS} if the \gls{mpi} command was successful, for instance. \gls{mpi} uses objects called communicators and groups to define which collection of processes can communicate with each other, and most \gls{mpi} routines require you to specify a communicator as an argument. {\tt MPI\_COMM\_WORLD} is the predefined communicator that includes all of your \gls{mpi} processes. Within each communicator, every process has a unique integer identifier assigned by the system when the process initializes. This rank is referred to as the ``task ID,'' and numbering begins at 0. The task ID is used by the programmer to specify the source and destination of messages, and these are often used conditionally. Most \gls{mpi} routines return an error code parameter, but the default behavior of an \gls{mpi} call is to abort if there is an error, so you actually would probably not be able to capture a return code unless successful ({\tt MPI\_SUCCESS}). 

Communication can be achieved using point-to-point operations, which pass messages between two different \gls{mpi} tasks. One task performs a send operation, while the other performs a receive operation. There are many combined send/receive routines, and any send routine can be paired with any type of receive routine. Because each send operation is not perfectly synchronized with its matching receive operation, the \gls{mpi} implementation has to deal with storing data when two tasks are out of sync. The \gls{mpi} implementation decides what happens to data in these situations. Usually, there is a system buffer area that is reserved to hold data in transit. This is referred to as ``message buffering.'' The system buffer space is managed entirely by the \gls{mpi} library, and can be exhausted fairly easily. This buffer region can exist on the sending and/or receiving sides. Alternative to the system buffer, where these messages are stored before they are ready to be sent/received is the application buffer, which contains the user-managed address space that contains your program variables. If data is stored in a system buffer upon receipt, that represents an asynchronous receive, whereas synchronous sends involve ``handshaking'' to conform the receive task sees the send.

More of the \gls{mpi} point-to-point routines can be used in either blocking or non-blocking modes. A blocking send routine will only ``return'' after it is safe to modify the application buffer (your send data) for reuse. Modifications will not impact the data intended for the receive task, though a blocking send routine does not mean that the data was actually received - it could be in the system buffer of the receiving processor. Non-blocking sends/receives will return almost immediately, and they do not wait for communication events to complete, such as message copying from user memory to system buffer space. Non-blocking communications are primarily used to overlap computation with communication to exploit additional performance gains. You would need to wait or probe to see whether the communication finished. The blocked command {\tt MPI\_Send()} could be thought of equivalently as being {\tt MPI\_Isend()} followed by {\tt MPI\_Wait()}. Using blocked communication also leads to deadlocks if you have two processors both attempting to send information to each other without one first receiving it, since the blocked communication will not return until the send has been completed, which cannot happen.

\gls{mpi} guarantees that messages will not overtake each other. For instance, if a processor sends out two messages, they will be received in chronological send order by the receiving processor. However, order rules do not apply if there are multiple threads participating in the communication operations. If you attempt to send two messages to a task, where that task only has one receive posted, then only one of those messages will be received. 

\gls{mpi} is a library, and hence it can be implemented in a variety of ways, the most common being MPICH (Argonne National Laboratory) and Open MPI. {\tt mpirun} determines how many copies of a process will start - you don't control how many processes are in the overall communicator at runtime - this is determined before you run. 

There are no shared variables - all communication and synchronization requires subroutine calls. Communication is performed as send-and-receive (pairwise) or as collective communication (broadcast, etc.). Collective communication could be performed using a binary tree, for example. It is very difficult to get data races in \gls{mpi}. All statements in the processes execute independently. 

\gls{mpi} is a language-independent \gls{api} that follows the \gls{spmd} paradigm, where the user writes a single program that runs on all distributed nodes, each operating on a different chunk of the data. The executable is launched on each of these nodes. 

\subsection{Environment Management}

This set of routines are used for specifying the \gls{mpi} execution environment. 

\begin{itemize}
\item {\tt MPI\_Abort(comm, errorcode)}: terminates all \gls{mpi} processes associated with the communicator. In most \gls{mpi} implementations, this will actually terminate all processes regardless of the communicator specified.
\item {\tt MPI\_Comm\_size(comm, \&size)}: returns the total number of \gls{mpi} processes in the specified communicator. If the communicator is {\tt MPI\_COMM\_WORLD}, then this represents the number of \gls{mpi} tasks available to your application.
\item {\tt MPI\_Comm\_rank(comm, \&rank)}: returns the rank of the calling \gls{mpi} process within the specified communicator. If a process becomes associated with other communicators, it will have a unique rank in each communicator.
\item {\tt MPI\_Finalize()}: terminates the \gls{mpi} execution environment
\item {\tt MPI\_Get\_processor\_name(\&name, \&resultlength)}: returns the processor name and the length of the name
\item {\tt MPI\_Get\_version(\&version, \&subversion)}: returns the version and subversion of the \gls{mpi} standard that is implemented by the library
\item {\tt MPI\_Init}: initializes the \gls{mpi} execution environment. This can only be called once in an \gls{mpi} program. For C programs, you can pass in command line arguments to all processes, though this isn't required ({\tt MPI\_Init(\&argc, \&argv)}).
\item {\tt MPI\_Initialized(\&flag)}: indicates whether the \gls{mpi} environment has been initialized using {\tt MPI\_Init}. 
\item {\tt MPI\_Wtick()}: returns the resolution of {\tt MPI\_Wtime}
\item {\tt MPI\_Wtime()}: returns an elapsed wall clock time on the calling processor
\end{itemize}

\subsection{Point-to-Point Communication}

The {\tt buffer} is the program address space that references the data that is to be sent or received. In most cases, this is simply the name of the variable that is sent/received. For C programs, this variable is passed by reference. The {\tt count} is the number of data elements of a particular type to be sent. For portability, \gls{mpi} predefines its elementary data types, prefaced by {\tt MPI}. For example, a signed int would be {\tt MPI\_INT}. The {\tt destination} is the process where the message should be delivered, and is written as the rank of the receiving process. For receive routines, you need to specify the {\tt source}, which is the originating process of the message, specified as the rank of the sending process. This could be set to {\tt MPI\_ANY\_SOURCE} to receive a message from any task. The {\tt tag} is an arbitrary, non-negative integer assigned by the programmer to uniquely identify a message. Send and receive operations should match message tags. For a receive operation, {\tt MPI\_ANY\_TAG} can be used to receive any message regardless of its tag. The {\tt comm}, or communicator, indicates the communication context, or set of processes for which the source or destination fields are valid. 

For a receive operation, the {\tt status} indicates the source and tag of the message, and is a pointer to a predefined structure {\tt MPI\_Status}. The {\tt request} is used by non-blocked send and receive operations. Because non-blocking send/receives can return before the requested system buffer space is obtained, the system issues a unique request number - the user uses this handle later in the wait routine to determine completion of the non-blocking operation. In C, this is a pointer to a predefined structure {\tt MPI\_Request}. 

\begin{itemize}
\item {\tt MPI\_Get\_count (\&status, datatype, \&count)}: returns the source, tag, and number of elements of datatype received
\item {\tt MPI\_Probe(source, tag, comm, \&status)}: performs a blocking test for a message
\item {\tt MPI\_Iprobe(source, tag, comm, \&flag, \&status)}: performs a non-blocking test for a message
\item {\tt MPI\_Send(\&buffer, count, type, destination, tag, comm)}: blocking send - the routine returns only after the application buffer in the sending task is free for reuse. Some implementations may use a synchronous send to implement the basic blocking send. 
\item {\tt MPI\_Isend(\&buffer, count, type, destination, tag, comm, request)}: non-blocking send. This identifies an area in memory to serve as a send buffer - processing continues immediately without waiting for the message to be coupied out from the application buffer. A handle is returned for assess the message status. The program should not modify the application buffer until subsequent calls to {\tt MPI\_Wait} or {\tt MPI\_Test} indicate that the send has been completed.
\item {\tt MPI\_Ssend(\&buffer, count, type, destination, tag, comm)}: synchronous blocking send - send a message and block until the application buffer in the sending task is free for reuse \textit{and} the destination process has started to receive the message. 
\item {\tt MPI\_Issend(\&buf, count, datatype, dest, tag, comm, \&request)}: non-blocking synchronous send
\item {\tt MPI\_Sendrecv(\&sendbuf, sendcount, sendtype, dest, sendtag, \&recvbuf, recvcount, recvtype, source, recvtag, comm \&status)}: send a message and post a receive before blocking (supply receive buffer at the same time as posting the send). This makes sure there is no deadlock. This will block until the sending application buffer is free for reuse and until the receiving application buffer contains the received message. This is not the same as a send following by a receive - it is more like a non-blocking send, a non-blockign receive, and a pair of waits. So, the send and receive occur in parallel. 
\item {\tt MPI\_Recv(\&buffer, count, type, source, tag, comm, status)}: blocking receive - the routine blocks until the request data is available in the application buffer of the receiving task. 
\item {\tt MPI\_IRecv(\&buffer, count, type, source, tag, comm, request)}: non-blocking receive - this identifies an area in memory to serve as a receive buffer. Processing continues immediately without waiting for the message to be received and copied into the application buffer. A handle is returned for assessing the message status. The program must use {\tt MPI\_Wait} or {\tt MPI\_Test} to determine when the non-blocking receive is completed. 
\item {\tt MPI_Test(\&request, \&flag, \&status)}: checks the status of a specified non-blocking send/receive operation. The {\tt flag} parameter returns 1 if the send/receive has completed, and 0 otherwise - this allows you to just check if something has completed, and do computation if not.
\item {\tt MPI\_Wait(request, status)}: waits for a send or receive to complete. In addition, you can use {\tt Waitany}, {\tt Waitall}, or {\tt Waitsome}.
\end{itemize}

\subsection{Collective Communication}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{figures/MPICommunication.pdf}
\caption{Forms of communication in \gls{mpi}.}
\end{figure}

Collective communication routines must involve all processes within the scope of a communicator. All processes are by default member in {\tt MPI\_COMM\_WORLD}. If one task in the communicator doesn't participate, then unexpected behavior, likely program failure, will occur - hence, if one process never calls the communicator, the program will hang, and hence collective communication is essentially a synchronization barrier as well. Collective communication routines don't require you to pass in a message tag. If you want only some processors to participate in a collective communication, then you need to first partition the overall processors into groups, and then attach the new groups to new communicators such that all of the processors in a new communicator are involved with the communication. Collective communication can also only be performed using \gls{mpi} predefined datatypes, and not with derived data types. The root process sends the data, and then everyone, including root, receives pieces of it.

\begin{itemize}
\item {\tt MPI\_Barrier(comm)}: barrier synchronization in a group - this is almost never used
\item {\tt MPI\_Bcast(\&buffer, count, datatype, root, comm)}: broadcasts (sends) a message from the process with rank {\tt root} to all other processes in the group. Broadcast is usually cheaper than doing \(n\) sends due to tree algorithms.
\item {\tt MPI\_Scatter(\&sendbuf, sendcnt, sendtype, \&recvbuf, recvcnt, recvtype, root, comm)}: distributes distinct messages from a single source task to each task in the group. The data must be contiguous in memory, and the message sizes uniform. Use {\tt MPI\_Scatterv} if the chunks of data you are sending are not contiguous in memory and are not all the same size. Data can also be distributed to processors in any order. 
\item {\tt MPI\_Scatterv(\&sendbuf, sendcnt, displacements, sendtype, \&recvbuf, recvtype, root, comm)}
\item {\tt MPI\_Gather(\&sendbuf, sendcnt, sendtype, \&recvbuf, recvcnt, recvtype, root, comm)}: gather distinct messages from each task in the group to a single destination task
\item {\tt MPI\_Allgather(\&sendbuf, sendcnt, sendtype, \&recvbuf, recvcnt, recvtype, comm)}: concatenation of data to all tasks in a group. Basically, each task in the group performs a one-to-all broadcasting operation within the group. 
\item {\tt MPI\_Reduce(\&sendbuf, \&recvbuf, count, datatype, op, root, comm)}: applies a reduction operation on all tasks in the group and places the result in one task. \gls{mpi} has several predefined reduction operations, but you can also define your own using the {\tt MPI\_Op\_create} routine. 
\item {\tt MPI\_Allreduce(\&sendbuf, \&recvbuf, count, datatype, op, comm)}: basically, perform a {\tt MPI\_Reduce}, but place the result in all tasks in the group. This is equivalent to a reduce followed by a broadcast. 
\item {\tt MPI\_Reduce\_scatter(\&sendbuf, \&recvbuf, recvcount, datatype, op, comm)}: perform a reduction on vector elements and distribute the segments of the resulting vector across all tasks. This is equivalent to a reduce followed by a scatter.
\item {\tt MPI\_Alltoall(\&sendbuf, sendcount, sendtype, \&recvbuf, recvcnt, recvtype, comm)}: Each task performs a scatter operation, sending a distinct message to all tasks in the group in order by index
\item {\tt MPI\_Scan(\&sendbuf, \&recvbuf, count, datatype, op, comm)}: perform a scan operation with respect to a reduction operation across a task group
\end{itemize}

\subsection{Derived Data Types}

\gls{mpi} provides the capability for you to define your own data structures based upon sequences of the \gls{mpi} primitive data types. Primitive data types are contiguous, while derived data types allow you to specify non-contiguous data in a convenient way and to treat it as if it were contiguous. You need to define these special \gls{mpi} data types if you want to share information that is not a traditional variable type, which could hurt performance if the datatypes are complicated. 

\begin{itemize}
\item {\tt MPI\_Type\_contiguous(count, oldtype, \&newtype)}: produces a new data type by making {\tt count} copies of an existing data type
\item {\tt MPI\_Type\_vector(count, blocklength, stride, oldtype, \&newtype)}: similar to contiguous, but this allows for regular gaps (stride) in the displacements
\item {\tt MPI\_Type\_indexed(count, blocklens[], offsets[], oldtype, \&newtype)}: an array of displacements of the input data type is provided as the map for the new data type
\item {\tt MPI\_Type\_extent(datatype, \&extent)}: returns the size in bytes of the specified data type, which is useful for the \gls{mpi} routines that require you to specify an offset in bytes
\item {\tt MPI\_Type\_commit(\&datatype)}: commits a new datatype to the system, which is required for all user-defined data types 
\item {\tt MPI\_Type\_free (\&datatype)}: deallocates the specified datatype object. This is important to prevent memory exhaustion
\end{itemize}

\subsection{Group and Communicator Management Routines}

A group is an ordered set of processes. In \gls{mpi}, a group is represented within memory as an object that is accessible to the programmer only by a handle. A group is always associated with a communicator object. Each process in a group is associated with a unique integer rank, beginning with zero. A communicator, on the other hand, is a group of processes that communicate with each other. All \gls{mpi} messages must specify a communicator. Like groups, communicators are represented within the system memory as objects and are accessible to the programmer only by handles. {\tt MPI\_COMM\_WORLD} is the communicator that comprises all tasks. From the programmer's perspective, a group and a communicator are the same thing. Groups and communicators allow you to enable collective communications across s subset of related tasks and organize tasks into task groups. They also permit safe communications while allowing the user flexibility in defining their own communication topologies. 

Groups/communicators are dynamic, and can be created and destroyed during program execution. Processes can be within more than one group or communicator, where they will have a unique rank within each group. The main difference between a group and a communicator is that within each group, the rank is that of the enclosing communicator, but once those groups are assigned to a new communicator, they have a second rank. 

\subsection{Virtual Topologies}

A virtual topology is a mapping or ordering of \gls{mpi} processes into a geometric shape - the two main topologies supported by \gls{mpi} are Cartesian grid and Graph. These topologies are entirely virtual, and there might not be any relationship to the physical structure of the machine or the process topology. Virtual topologies are constructed using \gls{mpi} groups and communicators, and must be programmed by the programmer. For example, a Cartesian topology would be best for applications that require four-way communication for grid-based data. You can also optimize process mapping based upon the physical characteristics of your machine so that you naturally avoid penalizing long-distance communications. 

\section{Partitioned Global Address Space Languages (UPC, UPC++)}

Programming in \gls{mpi} becomes very difficult when you don't know where a message to be received is coming from, since you'd have to use wildcards. \gls{pgas} systems work by allowing threads to directly read or write remote data, providing the conveniences of shared memory. The term \gls{pgas}, as opposed to shared memory, is used when the hardware does not actually reflect a shared memory system. The data is partitioned in that it is designated as local or global, helping you to achieve better locality. You can write or read data on another processor's memory without asking that processor to post a send or receive using message passing. Local pointers can only point to the memory on the node of the current processor. Global pointers can point to something on a node \textit{and} on a different node. Writes to values that are pointed to by global pointers is done under-the-hood using message passing as a data transfer through the network, but not as a send/receive (\gls{mpi}). \gls{pgas} languages can run large problems very well.

To program in \gls{upc}, you need to include the {\tt upc.h} header file. Any legal C program is also a legal \gls{upc} program. If you compile and run your code as \gls{upc} code with \(p\) threads, it will run \(p\) copies of the program. Just like \gls{mpi}, \gls{upc} is a \gls{spmd} programming model (code is ``copied'' to different processors and they each run the entire code serially). Normal C variables and objects are allocated in the private memory space for each thread. {\tt shared} variables are allocated only once, with thread 0 (and are stored in only one location (on processor 0)). Shared variables cannot have a dynamic lifetime, i.e. they cannot be defined in a function unless they are defined as {\tt static} - if thread 0 exits the function, then variables declared as shared will disappear, even though the other threads might still be trying to access them. 

\gls{pgas} languages suffer from race conditions and many other characteristics of shared memory programming. \gls{upc} has two basic forms of barriers - {\tt upc\_barrier} blocks until all other threads arrive, or split-phase barriers. With split-phase barriers, {\tt upc\_notify} specifies that this thread is ready for the barrier, and computation can be performed unrelated to the barrier. {\tt upc\_wait} then denotes that the thread should wait for others to be ready. This allows you to overlap computation with waiting, and achieve better load balancing if some processors finish much sooner than others. \gls{upc} also has locks for protecting shared data. The {\tt bupc\_collectivev.h} library includes collectives that prevent you needing to use shared variables.

Shared arrays are cyclic by default, and are spread over the threads. Shared arrays allow you to instead just modify the element belonging to your thread, and then at the end performing some aggregate operation - this can avoid the use of very large numbers of locks. Many \gls{upc} programs avoid these arrays in favor of directories. 

Instead of performing send/receives, you do put/get. Although \gls{pgas} seems closer to shared memory, it is more like how the hardware actually looks. For small messages, it is much better to use one-sided communication than send/receive with \gls{mpi}.

\end{flushleft}
\end{document}
